name: barney
namespace: barney

clusters:
# NOTE: dev deployments are on the "dev" branch of the barney ops repo.
# Do not add the dev cluster here.
- infra
- engprod-dr

barnzilla-config: &barnzilla_config
  parameters:
  - name: image.tag
    value: ${ARGOCD_APP_REVISION}--arch-x86_64

  source: &barnzilla_source
    repoURL: https://horseland-gerrit.infra.corp.arista.io/barnzilla
    path: helm/barnzilla

  syncPolicy:
    automated: {}

  values: &barnzilla_values
    image: &barnzilla_values_image
      repository: barney-docker.infra.corp.arista.io/code.arista.io/infra/barney/barnzilla

    config: &barnzilla_values_config
      # TODO: Once BAR and apiserver have auth enabled, we can remove this url override
      # and allow barnzilla to go through ambassador (which enables barnzilla->rits auth).
      rits:
        url: "http://barney-rats.barney:8880"

  clusters: &barnzilla_clusters
    infra: &barnzilla_infra
      service: &barnzilla_service
        domain: infra.corp.arista.io
        weight: 0
      podAnnotations:
        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/block.scrape: "true"
        profiles.grafana.com/mutex.scrape: "true"
      replicaCount: 2
      labels:
        environment: experimental
      redis:
        url: "rfs-barnzilla-redis.barney:26379"
      config:
        redis:
          sentinel: true
      resources:
        requests:
          cpu: 8
          memory: 20Gi
        limits:
          cpu: 8
          memory: 25Gi
    engprod-dr: &barnzilla_engprod_dr
      service: &barnzilla_engprod_dr_service
        domain: engprod-dr.corp.arista.io
        weight: 0
      image:
        repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/code.arista.io/infra/barney/barnzilla
      podAnnotations:
        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/block.scrape: "true"
        profiles.grafana.com/mutex.scrape: "true"
      replicaCount: 2
      labels:
        environment: experimental
      redis:
        url: "rfs-barnzilla-redis.barney:26379"
      config:
        redis:
          sentinel: true
      resources:
        requests:
          cpu: 1
          memory: 8Gi
        limits:
          cpu: 1.5
          memory: 8Gi

bsy-compat-config: &bsyCompatRoot
  source:
    repoURL: https://github.com/barney-ci/bsy-compat
    path: helm/bsy-compat

  parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}

  values: &bsyCompatValues
    replicaCount: 2
    image:
      repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/bsy-compat
      pullPolicy: IfNotPresent
    envSecrets:
      - bsy-compat-secrets
      - bsy-compat-dex-credentials

    config:
      # Switch to kebab-case because thos gets translated by the helm
      # chart to json5.
      allow-refs: true
      # Log what decision bsy-compat _would_ have made based on this policy.
      log-request-gate: true
      # Do not actually reject any requests -- we will inspect the logs to decide
      # whether our config is sufficient before actually enforcing it.
      apply-request-gate: true
      # Ref list generated based on an eos-trunk workspace from December 2024. More
      # refs will need to be added as we detect that older releases are using them.
      ref-gates:
        - ^arista%export-.*$
        - ^arista%test/.*$
        - ^arista%build/turbine.*$
        - ^arista.com/tools/breadman%export-breadman$
        - ^barney.ci/apiserver%static$
        - ^barney.ci/authspec%static$
        - ^barney.ci/authspec%test$
        - ^barney.ci/bst%static$
        - ^barney.ci/client%export$
        - ^barney.ci/client%static$
        - ^code.arista.io/atlas/pybsn%package$
        - ^code.arista.io/atlas/pybsn%test$
        - ^code.arista.io/atlas/ztn%export$
        - ^code.arista.io/autotest/workspace-adapter%export/bip$
        - ^code.arista.io/autotest/workspace-adapter%test/check$
        - ^code.arista.io/brcm/hsdk-diags%export-diags-6.5.26$
        - ^code.arista.io/brcm/hsdk%export.*$
        - ^code.arista.io/brcm/hsdk-.+%export.*$
        - ^code.arista.io/cv/AerisTest%export/AerisTest$
        - ^code.arista.io/cv/arista%export/CvpTest$
        - ^code.arista.io/cv/arista%test/CvpTest$
        - ^code.arista.io/cv/cvpi%with-env/cvp-ova-upload~releaseName//.*\+changeNum//.*$
        - ^code.arista.io/cv/cv-dut-ova-main%export/CvpDut$
        - ^code.arista.io/eos/tap-agg/gui%export$
        - ^code.arista.io/eos/eext-bundle%bundle-.*$
        - ^code.arista.io/eos/eext-bundle%list-unverified-sources$
        - ^code.arista.io/eos/eext/bazel%.*$
        - ^code.arista.io/eos/tools/tpm-hardware-auth%export$
        - ^code.arista.io/eos-web%eapi-export$
        - ^code.arista.io/fboss/in-eos%package_dnx_.*$
        - ^code.arista.io/fboss/in-eos%test/package_dnx_.*$
        - ^code.arista.io/fwd/sfe/bess$
        - ^code.arista.io/fwd/sfe/bess%bess-eos$
        - ^code.arista.io/fwd/sfe/dpdk%dpdk-eos$
        - ^code.arista.io/infra/abuildconfig%export-cli$
        - ^code.arista.io/infra/ardc%export-ardc$
        - ^code.arista.io/infra/ardc%export-rdamPdu$
        - ^code.arista.io/infra/arjobservicepyapi%export$
        - ^code.arista.io/infra/arjobservicepyapi%test$
        - ^code.arista.io/infra/autoabug%config$
        - ^code.arista.io/infra/barney/samples/barnzilla/tacc%export$
        - ^code.arista.io/infra/gerrit/hooks%static$
        - ^code.arista.io/infra/kennel%go/static$
        - ^code.arista.io/infra/kernelfeatures%export-cli$
        - ^code.arista.io/infra/logre%protos$
        - ^code.arista.io/infra/matt%export-mts-cli$
        - ^code.arista.io/infra/mrft%rollpoint-cli$
        - ^code.arista.io/infra/mrft%static$
        - ^code.arista.io/infra/mrft%tracefix-cli$
        - ^code.arista.io/infra/oauth2pyapi%export.*$
        - ^code.arista.io/infra/oauth2pyapi%test/check$
        - ^code.arista.io/infra/rdamservicepyapi%export$
        - ^code.arista.io/infra/rdamservicepyapi%test/check$
        - ^code.arista.io/infra/schedtestprotos%protos$
        - ^code.arista.io/infra/schedtest%static$
        - ^code.arista.io/infra/userprefs%docker/orca$
        - ^code.arista.io/infra/userprefs%userprefs-client$
        - ^code.arista.io/infra/wssecrets-client%export/python3-WsSecrets-Client$
        - ^code.arista.io/infra/wssecrets-client%test$
        - ^code.arista.io/mfw/in-eos%root-dir-with-agents-and-mfw$
        - ^code.arista.io/mgmt/agni-sdk%export/agni-sdk$
        - ^code.arista.io/mgmt/agni-sdk%test/AgniSDK$
        - ^code.arista.io/mgmt/cvp%export/CvpTest$
        - ^code.arista.io/mgmt/cvp%test.*$
        - ^code.arista.io/mgmt/wifi%export/.*$
        - ^code.arista.io/mgmt/wifi%test/.*$
        - ^code.arista.io/nm/dmf/swl%export/eos-ztntest$
        - ^code.arista.io/s4/tools%export$
        - ^code.arista.io/s4/tools%test.*$
        - ^code.arista.io/s4/tree-sitter-s4%tree-sitter-s4$
        - ^code.arista.io/security/openssh%export$
        - ^code.arista.io/strata/npl%export$
        - ^code.arista.io/strata/npl-td4x11%export$
        - ^code.arista.io/strata/npl-td4x9%export$
        - ^code.arista.io/sys/aboot/aboot11%insyde-bios$
        - ^code.arista.io/synce%export.*$
        - ^code.arista.io/synce%test.*$
        - ^code.arista.io/test/ondatra/featureprofiles%meta$
        - ^code.arista.io/timing/synce%export/.*$
        - ^code.arista.io/timing/synce%test/check$
        - ^code.arista.io/tools/a4c%export-build$
        - ^code.arista.io/tools/a4c%export-stest$
        - ^code.arista.io/tools/arelease%export/Arelease-BIP$
        - ^code.arista.io/tools/arelease%test/check$
        - ^code.arista.io/tools/baremetal-workload-identity%export/.*$
        - ^code.arista.io/tools/baremetal-workload-identity%test/run-pylint$
        - ^code.arista.io/tools/breadman%go/static$
        - ^code.arista.io/tools/breadman%static$
        - ^code.arista.io/tools/breadman%test/qubegen-failure-test$
        - ^code.arista.io/tools/breadman%test/qubegen-integration$
        - ^code.arista.io/tools/discourse-cli%export/ConvArsation$
        - ^code.arista.io/tools/formatdiff%source$
        - ^code.arista.io/tools/gitar%export$
        - ^code.arista.io/tools/python-hyperscan%python-hyperscan$
        - ^code.arista.io/tools/qube%export/qubefmt$
        - ^code.arista.io/tools/swiffer%swiffer$
        - ^code.arista.io/tools/swiffer%test$
        - ^code.arista.io/tools/terminattr-dumper%export-terminattr-dumper$
        - ^code.arista.io/tools/user-workspace/guest-access%export-userWorkspaceGuestAccess$
        - ^code.arista.io/tools/with-identity%export/python3-WithIdentity-Client$
        - ^code.arista.io/tools/with-identity%test$
        - ^code.arista.io/util/golang/ondatra-utils%fp-meta$
        - ^code.arista.io/util/linux/atfork%export$
        - ^github.com/aristanetworks/ArCloud%export-.*$
        - ^github.com/aristanetworks/geckodriver%geckodriver$
        - ^github.com/aristanetworks/quicktrace%export/QuickTrace$
        - ^github.com/aristanetworks/quicktrace%test/check$
        - ^github.com/aristanetworks/telegraf-mako%telegraf$
        - ^github.com/aristanetworks/telegraf-mako%test$
        - ^github.com/aristanetworks/trousers%export.*$
        - ^github.com/cli/cli%go/vet$
        - ^github.com/cli/cli%static$
        - ^github.com/protocolbuffers/protoscope%go/test$
        - ^github.com/protocolbuffers/protoscope%static$
        - ^github.com/trufflesecurity/trufflehog%static$
        - ^github.com/9elements/converged-security-suite%static$

  syncPolicy:
    automated: {}

  clusters:
    infra: &bsyCompatClustersInfra
      apiURL: https://barney-api.infra.corp.arista.io
      domain: infra.corp.arista.io
      replicaCount: 1
      service:
        weight: "0" # HEAD BSY_COMPAT_GREEN_BLUE_WEIGHT
        connectionTTLms: 300000
      resources:
        limits:
          cpu: 2
          memory: 1Gi
        requests:
          cpu: 2
          memory: 500Mi
    engprod-dr: &bsyCompatClustersEngprodDr
      apiURL: https://barney-api.engprod-dr.corp.arista.io
      domain: engprod-dr.corp.arista.io
      replicaCount: 1
      image:
        repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/bsy-compat
        pullPolicy: IfNotPresent
      service:
        weight: ""

api: &apiserver
  source:
    repoURL: https://github.com/barney-ci/apiserver
    path: helm/barney-api

  parameters:
  # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
  # is unable to substitute in a `values` string.  (Cargo culting this).
  - name: image.tag
    value: ${ARGOCD_APP_REVISION}--arch-x86_64

  values: &apiserver_values
    replicas: 1
    image:
      repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/apiserver
    environment:
      - name: OTEL_EXPORTER_JAEGER_AGENT_HOST
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName

    apiAuthentication:
      mode: "on"
      issuerURL: "https://auth.infra.corp.arista.io"
      deviceClientID: "diks355o3ywerbn2tyjbussxv"   # ardc-config dex: barney-command-line
      deviceClientEmailDomains:
        - "arista.com"
      credentialsClientIDs:
        - "fgzwfb3rmk3zyru3ss4iceory"                          # ardc-config dex: wssecrets-svc-a4ws-barney
        - "ezq7flyh7tmxnv4c5dhsdwkb6"                          # "barney-bsy-compat"
        - "piio67xak35wmp2l4ox54syp7"                          # "barney-github-bridge"
        - "p3laz5lo56oyu5edgbml7ixa6"                          # "barney-docker"
        - "n7fygxfopjbm6snclikbkcq4n"                          # "barney-b5-pinger"
        - "neqcthpkhc3q3w25j244o6gl6"                          # "bsn-jenkins"
        - &bbm-auth-client-id "adh2r55n7ax5nl5sqq4n3iouh"      # "bbm"
        - "zijctgdqleopn5rf5xackhcsb"                          # artools oauth2; curl -s https://discovery.infra.corp.arista.io/.well-known/artoolsoauth2-configuration | jq -r .cli_device.client_id
        - "qmlhqcjejkexrg5almgdbwg6t"                          # zuul-barney
        - "kdtt4zv5pcbmzkgo5vgmbogg5"                          # npl-barney
        - &bbm-test-auth-client-id "ypx26cevn4p6do4wkely4fu3x" # "bbm-test"
        - "uazh4cpeybzn76shn75gdcpng"                          # "barney-gh-b5-release-job"
        - "rk3izadxrakhk6mvu7o5klxty"                          # "bar-barney-oauth-client-credentials"

    githubApps:
      # A github API-backed feature may be disabled by removing its "appID" here.
      b5ClientDownloader:
        appID: "684913"
        installationID: "44913492"
      barneycontextGetter:
        appID: "718571"

    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/instance
                    operator: In
                    values:
                      - barney-api
              topologyKey: "kubernetes.io/hostname"

  syncPolicy:
    automated: {}

  clusters:
    infra: &api_infra_values
      domain: infra.corp.arista.io
      image:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/apiserver
      config:
        services:
          JobPost-barnzilla:
            method: POST
            path: /job
            cluster: worker-barnzilla
            response-codes-ranked:
              - 201 # Created
          JobPost-i386:
            method: POST
            path: /job
            cluster: worker-i386
            response-codes-ranked:
              - 201 # Created
        clusters:
          # Production x86_64 workers
          worker-x86_64:
            url: http://bsy
            balancer: weighted-one
            options: &bsy-worker-weighted-one-options
              probe-interval: 5s
              probe-history: 65
              chooser-factor: 1e+5
              exprn-debug: 1
              metrics: &bsy-worker-weighted-one-options-metrics
                one: '1.0'
                # pending+running jobs cap out at around 40, so the range of this metric is [0,40].
                topLevelJobs: >-
                  sumFloat64
                    (sumGauges
                      (selectMany
                        "status=pending"
                        (metric "barney_bsy_jobs" (now))))
                    (sumGauges
                      (selectMany
                        "status=running"
                        (metric "barney_bsy_jobs" (now))))
                # When the cluster is under heavy load, nodes have been observed to retire 200 jobs per 5 minute interval. So
                # the range of the `monoFloat64` subexpression is about [0,200). We then scale it to [0, 10) because we want
                # this to be around the same order as te topLevelJobs metric. We then sift it to [1, 11) because the fallback
                # behavior when there is not 5 minutes of history is to omit the expression entirely which is equivalent to
                # a value of 1 --- we shift the range so that the lower bound matches the fallback value so that fallback
                # nodes get no bonus from being in that state.
                jobsCompleted5mScaled: >-
                  (sumFloat64
                    1.0
                    (productFloat64
                      0.05
                      (monoFloat64
                        (sumFloat64
                          (counter (labels "transit_from=running,transit_to=passed" (metric "barney_bsy_job_transit" (now))))
                          (counter (labels "transit_from=running,transit_to=failed" (metric "barney_bsy_job_transit" (now))))
                        )
                        (sumFloat64
                          (counter (labels "transit_from=running,transit_to=passed" (metric "barney_bsy_job_transit" (back 300))))
                          (counter (labels "transit_from=running,transit_to=failed" (metric "barney_bsy_job_transit" (back 300))))
                        ))))
              # product will ignore any arguments which cannot be evaluated. By adding in "(metric "one")" we create
              # an argument which will always evaluate even if the node is missing some metrics (e.g. it's not fully
              # initialized yet). The usage of "Default" here is meant to punish nodes that have no recent data.
              weighter: >-
                normalize (product
                  (metric "one")
                  (reciprocal (add 1 (default (metric "topLevelJobs") 10.0)))
                  (default (metric "jobsCompleted5mScaled") 1)
                )
          worker-x86_64-random-all:
            url: http://bsy
            balancer: random-all
          worker-x86_64-random-one:
            url: http://bsy
            balancer: random-one

          # Same backing subservice as worker-x86_64 but with a different
          # weighter expression; intended to test the impact of various
          # weighter expressions on job latency.
          worker-x86_64-weighter-expr-test:
            url: http://bsy
            balancer: weighted-one
            options:
              <<: *bsy-worker-weighted-one-options

          # Production i386 workers
          worker-i386:
            url: http://bsy
            balancer: weighted-one
            options:
              <<: *bsy-worker-weighted-one-options
          worker-i386-random-all:
            url: http://bsy

            balancer: random-all

          # Production aarch64 workers
          worker-aarch64:
            url: http://bsy-aarch64
            balancer: weighted-one
            options:
              <<: *bsy-worker-weighted-one-options
          worker-aarch64-random-all:
            url: http://bsy-aarch64
            balancer: random-all
          worker-aarch64-random-one:
            url: http://bsy-aarch64
            balancer: random-one

          # Head x86_64, i386, and aarch64 workers
          worker-x86_64-head:
            url: http://barney-bsy-head
            balancer: random-one
          worker-x86_64-head-random-all:
            url: http://barney-bsy-head
            balancer: random-all
          worker-i386-head:
            url: http://barney-bsy-head
            balancer: random-one
          worker-i386-head-random-all:
            url: http://barney-bsy-head
            balancer: random-all
          worker-aarch64-head:
            url: http://barney-bsy-head-aarch64
            balancer: random-one
          worker-aarch64-head-random-all:
            url: http://barney-bsy-head-aarch64
            balancer: random-all

          # Barnzilla is a special worker that implements the
          # POST /jobs interface by creating a MUT to convey the requested
          # repomap into a perforce proejct.
          worker-barnzilla:
            url: http://barnzilla-bsy-worker
            balancer: random-all
          job-status:
            url: http://barney-job-compat
            balancer: random-all
          job-status-head:
            url: http://barney-job-compat-head
            balancer: random-all
          cache:
            url: http://barney-cache
            balancer: random-all
          cache-head:
            url: http://barney-cache-head
            balancer: random-all
          build:
            url: http://barney-build-manager-prod-infra-api
            balancer: random-all
          build-head:
            url: http://barney-build-manager-head-infra-api
            balancer: random-all
          bus:
            url: http://barney-bbm
            balancer: random-one
          jobstore-head:
            url: http://barney-jobstore-head
            balancer: random-all


      service:
        annotations:
          # API_GREEN_BLUE_WEIGHT
          getambassador.io/config: |
            ---
            apiVersion: getambassador.io/v2
            kind:  Mapping
            name:  barney-api_top_http_mapping
            ambassador_id: [internal_proxy]
            bypass_auth: true
            host: barney-api.infra.corp.arista.io
            timeout_ms: 0
            idle_timeout_ms: 36000000
            prefix: /
            service: barney-api.barney.svc.cluster.local.:80
            weight: 0
      replicaCount: 2
      autoscaling:
        enabled: false # note: not supported at the moment on infra cluster
        targetCPUUtilizationPercentage: 60
        targetMemoryUtilizationPercentage: 60
        minReplicas: 2
        maxReplicas: 10
      resources: # Approximage usage measured on September 2024
        limits:
          cpu: 3
          memory: 4Gi
        requests:
          cpu: 3
          memory: 3Gi
      secrets:
        mount: apiserver-secrets

    engprod-dr: &api_dr_values
      domain: engprod-dr.corp.arista.io
      image:
        repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/apiserver
      apiAuthentication:
        issuerURL: "https://auth.engprod-dr.corp.arista.io"

      config:
        services:
          JobPost-barnzilla:
            method: POST
            path: /job
            cluster: worker-barnzilla
            response-codes-ranked:
              - 201 # Created
          JobPost-aarch64: null
          GetBuses: null
          PostBus: null
          GetBus: null
          UpdateBus: null
          DeleteBus: null
          BusJobs: null
          BusRepos: null
          GetBusRepo: null
          PutBusRepo: null
          DeleteBusRepo: null
          BusRepomap: null
          BusSSH: null
          GetBuild: null
          PutBuild: null
          SearchBuilds: null
        clusters:
          worker-x86_64-random-all: &engprod-dr-worker-x86_64-random-all
            url: http://barney-bsy
            balancer: random-all
          worker-x86_64: *engprod-dr-worker-x86_64-random-all
          worker-barnzilla:
            url: http://barney-barnzilla-red
            balancer: random-all
          job-status:
            url: http://barney-job-compat
            balancer: random-all
          cache:
            url: http://barney-bsy
            balancer: random-all


      service:
        annotations:
          getambassador.io/config: |
            ---
            apiVersion: getambassador.io/v2
            kind:  Mapping
            name:  barney-api_top_http_mapping
            ambassador_id: [internal_proxy]
            bypass_auth: true
            host: barney-api.engprod-dr.corp.arista.io
            timeout_ms: 0
            idle_timeout_ms: 36000000
            prefix: /
            service: barney-api.barney.svc.cluster.local.:80
            ---
            apiVersion: getambassador.io/v2
            kind:  Mapping
            name:  barney-api_top_http_port_mapping
            ambassador_id: [internal_proxy]
            bypass_auth: true
            host: barney-api.barney:443
            timeout_ms: 0
            idle_timeout_ms: 36000000
            prefix: /
            service: barney-api.barney.svc.cluster.local.:80
            ---
            apiVersion: getambassador.io/v2
            kind:  Mapping
            name:  barney-api_direct_http_mapping
            ambassador_id: [internal_proxy]
            bypass_auth: true
            host: barney-api.barney
            timeout_ms: 0
            idle_timeout_ms: 36000000
            prefix: /
            service: barney-api.barney.svc.cluster.local.:80
            ---
            apiVersion: getambassador.io/v2
            kind:  Mapping
            name:  barney-api_direct_http_port_mapping
            ambassador_id: [internal_proxy]
            bypass_auth: true
            host: barney-api.engprod-dr.corp.arista.io:443
            timeout_ms: 0
            idle_timeout_ms: 36000000
            prefix: /
            service: barney-api.barney.svc.cluster.local.:80
      replicaCount: 2
      secrets:
        mount: apiserver-secrets




services:
  api-blue:
    <<: *apiserver

    # clusterSource extends "source" above with per-cluster parameters.
    clusterSource:
      infra:
        targetRevision: 790882e9069b3e97aa3aeaf8ebd11abbbd8b5d32
      engprod-dr:
        targetRevision: 790882e9069b3e97aa3aeaf8ebd11abbbd8b5d32

    clusters:
      infra:
        <<: *api_infra_values
        replicaCount: 2
        service:
          annotations:
            # API_GREEN_BLUE_WEIGHT
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-api-blue_top_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barney-api.infra.corp.arista.io
              timeout_ms: 0
              grpc: true
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-api-blue.barney.svc.cluster.local.:80
              weight: 0
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-api-blue_direct_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barney-api-blue.infra.corp.arista.io
              timeout_ms: 0
              grpc: true
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-api-blue.barney.svc.cluster.local.:80
      engprod-dr:
        <<: *api_dr_values
        replicaCount: 2
        service:
          annotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-api-blue_top_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barney-api.engprod-dr.corp.arista.io
              timeout_ms: 0
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-api-blue.barney.svc.cluster.local.:80
              weight: 0
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-api-blue_direct_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barney-api-blue.engprod-dr.corp.arista.io
              timeout_ms: 0
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-api-blue.barney.svc.cluster.local.:80

  api-green:
    <<: *apiserver

    # clusterSource extends "source" above with per-cluster parameters.
    clusterSource:
      infra:
        targetRevision: 790882e9069b3e97aa3aeaf8ebd11abbbd8b5d32
      engprod-dr:
        targetRevision: 790882e9069b3e97aa3aeaf8ebd11abbbd8b5d32

    clusters:
      infra:
        <<: *api_infra_values
        replicaCount: 8
        service:
          annotations:
            # API_GREEN_BLUE_WEIGHT
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-api-green_top_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barney-api.infra.corp.arista.io
              timeout_ms: 0
              grpc: true
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-api-green.barney.svc.cluster.local.:80
              weight: 100
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-api-green_direct_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barney-api-green.infra.corp.arista.io
              timeout_ms: 0
              grpc: true
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-api-green.barney.svc.cluster.local.:80
      engprod-dr:
        <<: *api_dr_values
        replicaCount: 2
        service:
          annotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-api-green_top_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barney-api.engprod-dr.corp.arista.io
              timeout_ms: 0
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-api-green.barney.svc.cluster.local.:80
              weight: 0
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-api-green_direct_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barney-api-green.engprod-dr.corp.arista.io
              timeout_ms: 0
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-api-green.barney.svc.cluster.local.:80


  api-internal-cache:
    <<: *apiserver

    # clusterSource extends "source" above with per-cluster parameters.
    clusterSource:
      infra:
        targetRevision: 790882e9069b3e97aa3aeaf8ebd11abbbd8b5d32
      engprod-dr:
        targetRevision: 790882e9069b3e97aa3aeaf8ebd11abbbd8b5d32

    values:
      <<: *apiserver_values

      apiAuthentication:
        mode: "off"

    clusters:
      infra:
        <<: *api_infra_values
        service:
          annotations:

      engprod-dr:
        <<: *api_dr_values
        service:
          annotations:

  api-head: &api_head_service
    source:
      repoURL: https://github.com/barney-ci/apiserver
      targetRevision: pull/300/head
      path: helm/barney-api

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.  (Cargo culting this).
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-x86_64

    values: &api_head_values
      <<: *apiserver_values
      image:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/apiserver

      config:
        services:
          JobPost-barnzilla:
            method: POST
            path: /job
            cluster: worker-barnzilla
            response-codes-ranked:
              - 201 # Created
          JobStatus:
            cluster: job-status-head
          Snapshot:
            cluster: cache-service
        clusters:
          worker-x86_64:
            url: http://bsy
            balancer: weighted-one
            options:
              metrics:
                one: '1.0'
                jobsPassed10m: >-
                  (monoFloat64
                    (counter (labels
                      "transit_from=running,transit_to=passed"
                      (metric "barney_bsy_job_transit" (now))))
                    (counter (labels
                      "transit_from=running,transit_to=passed"
                      (metric "barney_bsy_job_transit" (back 600)))))
                runningJobs: >-
                  (gauge (status
                    "running"
                    (metric "barney_bsy_jobs" (now))))
                pendingJobs:  >-
                  (gauge (status
                    "pending"
                    (metric "barney_bsy_jobs" (now))))
                freeCPU: >-
                  (gauge (labels "quota=task.quotaNumCPU"
                    (metric "task_qsem_quotas_current" (now))))
              # product will ignore any arguments which cannot be evaluated. By adding in "(metric "one")" we create
              # an argument which will always evaluate even if the node is missing some metrics (e.g. it's not fully
              # initialized yet).
              # The "topLevelJobs" argument is meant to favor nodes that have fewer running+pending jobs (less busy)
              # The "jobsCompleted5mScaled" argument is meant to favor nodes that have been retiring jobs quickly. Note
              # that this expression is weighted; normally we see 5 minute job completion in the range of [0,250),
              # but the expression is weighted to run from [0,12.5) which is much more in line with the [0,40) range
              # we expect to see from the topLevelJobs field. That is, the expected contribution from jobsCompleted
              # to be about a quarter of te contribution from topLevelJobs. Because this expression evaluates to `1`
              # when there is not enough history, we actually want to shift the entire range rightward so that new
              # endpoints appear as if they have done 0 work.
              weighter: >-
                scale 0.0 1.0 (sum
                  (metric "one")
                  (default (neg (scale 0.0 200.0 (product (metric "pendingJobs") (metric "pendingJobs")))) -200.0)
                  (default (neg (scale 0.0 100.0 (product (metric "runningJobs") (metric "runningJobs")))) -100.0)
                  (default (scale 0.0 30.0 (metric "freeCPU")) 0)
                  (default (scale 0.0 10.0 (metric "jobsPassed10m")) 0)
                )
              probe-interval: 5s
              probe-history: 65
              chooser-factor: 1e+5
              exprn-debug: 1
          worker-x86_64-random-all:
            url: http://bsy
            balancer: random-all
          worker-aarch64:
            url: http://bsy-aarch64
            balancer: random-one
          worker-aarch64-random-all:
            url: http://bsy-aarch64
            balancer: random-all
          worker-barnzilla:
            url: http://barnzilla-bsy-worker
            balancer: random-all
          worker-x86_64-head:
            url: http://barney-bsy-head
            balancer: random-all
          worker-x86_64-head-random-all:
            url: http://barney-bsy-head
            balancer: random-all
          worker-aarch64-head:
             url: http://barney-bsy-head-aarch64
             balancer: random-one
          worker-aarch64-head-random-all:
             url: http://barney-bsy-head-aarch64
             balancer: random-all
          job-status:
            url: http://barney-job-compat
            balancer: random-all
          job-status-head:
            url: http://barney-job-compat-head
            balancer: random-all
          cache:
            url: http://barney-cache
            balancer: random-all
          cache-service:
            url: http://barney-cache-head
            balancer: custodian
            options:
              custodian-path-template: '/snapshot/{{.hash}}/custodians'
              custodian-scheme: "http"
          cache-head:
            url: http://barney-cache-head
            balancer: random-all
          build:
            url: http://barney-build-manager-prod-infra-api
            balancer: random-all
          build-head:
            url: http://barney-build-manager-head-infra-api
            balancer: random-all
          bus:
            url: http://barney-bbm-head
            balancer: random-one
          jobstore:
            url: http://barney-jobstore
            balancer: random-all
          jobstore-head:
            url: http://barney-jobstore-head
            balancer: random-all

    clusters:
      infra: &api_head_infra
        domain: infra.corp.arista.io
        service:
          annotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-api-head_top_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barney-api-head.infra.corp.arista.io
              timeout_ms: 0
              grpc: true
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-api-head.barney.svc.cluster.local.:80
        replicaCount: 1
        nodeSelector:
          node-role.kubernetes.io/barney-aux: barney-aux
        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule
        secrets:
          mount: apiserver-secrets
      engprod-dr:
        disable: true

    syncPolicy:
      automated: {}

  bsy-compat-green:
    <<: *bsyCompatRoot

    # clusterSource extends "source" above with per-cluster parameters.
    clusterSource:
      infra:
        targetRevision: 302f632d15883c787620d16542a28fa5ee893171
      engprod-dr:
        targetRevision: 302f632d15883c787620d16542a28fa5ee893171

    values:
      <<: *bsyCompatValues

    clusters:
      infra:
        <<: *bsyCompatClustersInfra
        replicaCount: 8
        service:
          weight: "75" # GREEN -- BSY_COMPAT_GREEN_BLUE_WEIGHT
          connectionTTLms: 300000
      engprod-dr:
        <<: *bsyCompatClustersEngprodDr

  bsy-compat-blue:
    <<: *bsyCompatRoot

    # clusterSource extends "source" above with per-cluster parameters.
    clusterSource:
      infra:
        targetRevision: 302f632d15883c787620d16542a28fa5ee893171
      engprod-dr:
        targetRevision: 302f632d15883c787620d16542a28fa5ee893171

    values:
      <<: *bsyCompatValues

    clusters:
      infra:
        <<: *bsyCompatClustersInfra
        replicaCount: 4
        service:
          weight: "25" # BLUE -- BSY_COMPAT_GREEN_BLUE_WEIGHT
          connectionTTLms: 300000
      engprod-dr:
        <<: *bsyCompatClustersEngprodDr

  bsy-compat-head:
    <<: *bsyCompatRoot

    source:
      repoURL: https://github.com/barney-ci/bsy-compat
      targetRevision: HEAD
      path: helm/bsy-compat

    values:
      <<: *bsyCompatValues

      nameOverride: bsy-head
      image:
        repository: barney-docker.infra.corp.arista.io/barney.ci/bsy-compat
        pullPolicy: IfNotPresent

    clusters:
      infra:
        <<: *bsyCompatClustersInfra
        apiURL: http://barney-api-head.barney
      engprod-dr:
        disable: true

  btm-head:
    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/topicmanager
      targetRevision: HEAD
      path: helm/btm

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}

    syncPolicy:
      automated:
        prune: true

    valueFiles:
      - ../../deployment/btm-topic-finders.yaml

    values: &btm_head_values
      fullnameOverride: btm-head
      fullname: btm-head
      varnish:
        enabled: false
        vclConfigmap: btm-head-varnish
      image:
        repository: barney-docker.infra.corp.arista.io/barney.ci/topicmanager
      podAnnotations:
        maintainer: barney-dev

        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/cpu.port: "8081"
        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/memory.port: "8081"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/goroutine.port: "8081"
      bsy:
        url: https://bsy.infra.corp.arista.io
      influx:
        address: planck.sjc.aristanetworks.com:8107
      kafka:
        address: barney-kafka-cluster-kafka-bootstrap.barney:9092
        groupId: "btm-head"
        barneyTopicsGitTopic: barney-topics-git-head
        barneyTopicsTopic: barney-topics-head
      repoMetadata:
        url: http://repometadata.barney:8080
      service:
        weight: 0
        shadow: false
        domain: infra.corp.arista.io
      firestore:
        project: sw-infra-barney-dev
        credentials: /etc/secrets/btm-head-secrets/firestore.json
      secrets:
        - btm-head-secrets
        - barney-ci-app
      resources:
        limits:
          cpu: "2"
          memory: "4Gi"
        requests:
          cpu: "1"
          memory: "2Gi"

    clusters:
      infra:
        enabled: true

  btm-red:
    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/topicmanager
      targetRevision: c530e1bafe4e44f8635fa4579d8393bfb720816f
      path: helm/btm

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--arch-x86_64

    syncPolicy:
      automated:
        prune: true

    valueFiles:
      - ../../deployment/btm-topic-finders.yaml

    values:
      <<: *btm_head_values
      image:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/topicmanager
      fullnameOverride: btm-red
      fullname: btm-red
      varnish:
        enabled: true
        vclConfigmap: btm-red-varnish
        nameOverride: btm-red-varnish
      # btm_green_red This deployment is SHRINKING
      replicaCount: 0
      kafka:
        address: barney-kafka-cluster-kafka-bootstrap.barney:9092
        groupId: "btm"
        barneyTopicsGitTopic: barney-topics-git
        barneyTopicsTopic: barney-topics
      service:
        weight: 0
        domain: infra.corp.arista.io
      firestore:
        project: sw-infra-barney
        credentials: /etc/secrets/btm-secrets/firestore.json
      secrets:
        - btm-secrets
        - barney-ci-app

    clusters:
      infra:
        disable: false
      engprod-dr:
        disable: true
        image:
          repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/btm
        service:
          domain: engprod-dr.corp.arista.io
        firestore:
          project: sw-engprod-dr-barney

  btm-green:
    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/topicmanager
      targetRevision: e7702f761cfdfc35cd4aae57da7324a02ceb6f2d
      path: helm/btm

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--arch-x86_64

    syncPolicy:
      automated:
        prune: true

    valueFiles:
      - ../../deployment/btm-topic-finders.yaml

    values:
      <<: *btm_head_values
      image:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/topicmanager
      fullnameOverride: btm-green
      fullname: btm-green
      varnish:
        enabled: true
        vclConfigmap: btm-green-varnish
        nameOverride: btm-green-varnish
      # btm_green_red This deployment is GROWING
      replicaCount: 4
      kafka:
        address: barney-kafka-cluster-kafka-bootstrap.barney:9092
        groupId: "btm"
        barneyTopicsGitTopic: barney-topics-git
        barneyTopicsTopic: barney-topics
      service:
        weight: 100
        domain: infra.corp.arista.io
      firestore:
        project: sw-infra-barney
        credentials: /etc/secrets/btm-secrets/firestore.json
      secrets:
        - btm-secrets
        - barney-ci-app

    clusters:
      infra:
        disable: false
      engprod-dr:
        disable: true
        image:
          repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/btm
        service:
          domain: engprod-dr.corp.arista.io
        firestore:
          project: sw-engprod-dr-barney

  job-compat:
    source:
      repoURL: https://github.com/barney-ci/job-compat-service
      path: helm/job-compat

    # clusterSource extends "source" above with per-cluster parameters.
    clusterSource:
      infra:
        targetRevision: edfde4379bfed5d842f3613a5115742f624b6233
      engprod-dr:
        targetRevision: 22eebc4ef03aae33ec997600df3a40300ff686b8

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--arch-x86_64

    values:
      image:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/job-compat-service

      gcImage:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/barney
        tag: 6dd4912183b7bc9022b806c6fccdb3aebd73986c--arch-x86_64

      # It's important that job-compat and job-compat-head run on
      # different nodes, because otherwise they will fight over /b5.
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - barney-job-compat-infra
                - barney-job-compat-head-infra
            topologyKey: "kubernetes.io/hostname"

      deployment: &jobcompat-cfg
        kafkaHost: barney-kafka-cluster-kafka-brokers.barney
        kafkaTopics:
          - barney-user-events
          - barney-top-level-jobs
          - barney-job-details
          - barney-barnzilla-jobs
        workersPerTopic: 8 # should match or exceed number of partitions
        groupIdFromNode: true # The groupID is the name of the node

      statefulSet: *jobcompat-cfg

      podAnnotations:
        maintainer: barney-dev
        prometheus.io/scrape: "true"
        prometheus.io/port: "3000"

        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/cpu.port: "3000"
        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/memory.port: "3000"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/goroutine.port: "3000"
        profiles.grafana.com/mutex.scrape: "true"
        profiles.grafana.com/mutex.port: "3000"
        profiles.grafana.com/block.scrape: "true"
        profiles.grafana.com/block.port: "3000"

    syncPolicy:
      automated: {}

    clusters:
      infra:
        replicaCount: 2

        deployment:
          bsyaddr: https://bsy.infra.corp.arista.io/

        cache:
          nodePath: /data2/b5cache

        service:
          annotations:
        nodeSelector:
          node-role.kubernetes.io/barney-aux: barney-aux
        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule

      engprod-dr:
        replicaCount: 1

        image:
          repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/job-compat-service

        gcImage:
          repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/barney
          tag: 6dd4912183b7bc9022b806c6fccdb3aebd73986c--arch-x86_64

        statefulSet:
          bsyaddr: https://bsy.engprod-dr.corp.arista.io/

        cache:
          volumeClaim:
            enabled: true
            storageClassName: standard-rwo
            size: 200Gi

        podSecurityContext:
          fsGroup: 10000
          fsGroupChangePolicy: "OnRootMismatch"

        service:
          annotations:

  job-compat-head:
    source:
      repoURL: https://github.com/barney-ci/job-compat-service
      targetRevision: HEAD
      path: helm/job-compat

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--arch-x86_64

    values:
      image:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/job-compat-service

      # It's important that all instances of job-compat-** run on
      # different nodes, because otherwise they will fight over /b5.
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app.kubernetes.io/instance
                operator: In
                values:
                - barney-job-compat-infra
                - barney-job-compat-head-infra
            topologyKey: "kubernetes.io/hostname"

      deployment:
        kafkaHost: barney-kafka-cluster-kafka-brokers.barney
        kafkaTopics:
          - barney-user-events
          - barney-top-level-jobs
          - barney-job-details
          - barney-barnzilla-jobs
        workersPerTopic: 8 # should match or exceed number of partitions
        groupIdFromNode: true # The groupID is the name of the node

      podAnnotations:
        maintainer: barney-dev
        prometheus.io/scrape: "true"
        prometheus.io/port: "3000"

        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/cpu.port: "3000"
        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/memory.port: "3000"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/goroutine.port: "3000"
        profiles.grafana.com/mutex.scrape: "true"
        profiles.grafana.com/mutex.port: "3000"
        profiles.grafana.com/block.scrape: "true"
        profiles.grafana.com/block.port: "3000"

    syncPolicy:
      automated: {}

    clusters:
      infra:

        replicaCount: 1

        deployment:
          bsyaddr: https://bsy-head.infra.corp.arista.io/

        cache:
          nodePath: /data2/b5cache

        gcImage:
          repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/barney
          tag: 6dd4912183b7bc9022b806c6fccdb3aebd73986c--arch-x86_64

        service:
          annotations:
        nodeSelector:
          node-role.kubernetes.io/barney-aux: barney-aux
        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule
      engprod-dr:
        disable: true

  aeris-gc:
    # Service removes old objects from aeris. TTL is configured
    # on a per-path basis.
    source:
      repoURL: https://github.com/barney-ci/aeris
      targetRevision: HEAD
      path: helm/aeris-gc

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}

    syncPolicy:
      automated: {}

    clusters:
      infra:
        secrets:
          mount: bsy-aeris-mounted-secrets
        pod:
          addEnv:
          - name: AERIS_SESSION_TOKEN_PATH
            value: /etc/secrets/aeris-session-token
        podAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "3000"
          prometheus.io/scrape: "true"

          profiles.grafana.com/cpu.scrape: "true"
          profiles.grafana.com/cpu.port: "3000"
          profiles.grafana.com/memory.scrape: "true"
          profiles.grafana.com/memory.port: "3000"
          profiles.grafana.com/goroutine.scrape: "true"
          profiles.grafana.com/goroutine.port: "3000"

        replicaCount: 1
        config:
          workers: 5
          targets:
            jobs1:
              object-path-pattern: "/jobs/J1/*"
              ttl: &TTL "96h"
        service:
          annotations:
            getambassador.io/config: |
              apiVersion: getambassador.io/v2
              kind: Mapping
              name: barney-aeris-gc_direct_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-aeris-gc.infra.corp.arista.io
              timeout_ms: 36000000
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-aeris-gc.barney:3000
      engprod-dr:
        disable: true

  jobwriter-topjobs-redis:
    source:
      repoURL: https://github.com/aristanetworks/barney-ops
      targetRevision: HEAD
      path: helm-redis

    syncPolicy:
      automated: {}

    clusters:
      infra:
        name: "topjobs-redis"
        replicas: 3
        image: redis:7.0.15-alpine
        customConfig:
          - "maxmemory 3900MB"
          - "maxmemory-policy allkeys-lru"
        resources:
          requests:
            memory: "4Gi"
          limits:
            memory: "4Gi"

  jobwriter-redis:
    source:
      repoURL: https://github.com/barney-ci/aeris
      path: helm/aeris-jobwriter
      targetRevision: refs/heads/a2n/redis-writer

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.  (Cargo culting this).
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-x86_64

    syncPolicy:
      automated: {}

    clusters:
      infra:
        replicaCount: 4
        pod:
          addArgs:
          - --topic=barney-top-level-jobs
          - --topic=barney-barnzilla-jobs
          - --redis-client-type=sentinel
          - --redis-host=rfs-topjobs-redis.barney:26379
          - --group-id=jobwriter-redis
          - --redis-record-ttl=96h
        podAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "3000"
          prometheus.io/scrape: "true"

          profiles.grafana.com/cpu.scrape: "true"
          profiles.grafana.com/cpu.port: "3000"
          profiles.grafana.com/memory.scrape: "true"
          profiles.grafana.com/memory.port: "3000"
          profiles.grafana.com/goroutine.scrape: "true"
          profiles.grafana.com/goroutine.port: "3000"
      engprod-dr:
        disable: true

  jobdetails-redis:
    source:
      repoURL: https://github.com/aristanetworks/barney-ops
      targetRevision: HEAD
      path: helm-redis

    syncPolicy:
      automated: {}

    clusters:
      infra:
        name: "jobdetails-redis"
        replicas: 3
        image: redis:7.0.15-alpine
        customConfig:
          - "maxmemory 3900MB"
          - "maxmemory-policy allkeys-lru"
        resources:
          requests:
            memory: "4Gi"
          limits:
            memory: "4Gi"

  jobdetails-dragonfly:
    # Using the helm chart from dragonflyDB just to test whether data
    # tiering is perofrmant for our purposes.
    source:
      repoURL: https://github.com/dragonflydb/dragonfly
      # https://github.com/dragonflydb/dragonfly/releases/tag/v1.20.1
      targetRevision: 501b7f7b4fb049de2a8a5fff15d945cd7da1046a
      path: contrib/charts/dragonfly

    syncPolicy:
      automated: {}

    clusters:
      infra:
        image:
          tag: v1.20.1
        podAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "6379"
          prometheus.io/scrape: "true"
        extraArgs:
          - "--cache_mode=true"
        podSecurityContext:
          fsGroup: 2000
        securityContext:
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
        resources:
          requests:
            memory: "15Gi"
          limits:
            memory: "15Gi"
      engprod-dr:
        disable: true

  jobwriter-redis-jobdetails:
    source:
      repoURL: https://github.com/barney-ci/aeris
      path: helm/aeris-jobwriter
      targetRevision: refs/heads/a2n/redis-writer

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.  (Cargo culting this).
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-x86_64

    syncPolicy:
      automated: {}

    clusters:
      infra:
        replicaCount: 4
        pod:
          addArgs:
          - --topic=barney-top-level-jobs
          - --topic=barney-job-details
          - --topic=barney-barnzilla-jobs
          - --redis-client-type=standalone
          - --redis-host=barney-jobdetails-dragonfly.barney:6379
          - --filter-instance=barney-bsy-27
          - --filter-instance=barney-bsy-26
          - --filter-instance=barney-bsy-25
          - --filter-instance=barney-bsy-24
          - --group-id=jobwriter-redis-jobdetails
          - --redis-record-ttl=6h
          - --num-workers=4
        podAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "3000"
          prometheus.io/scrape: "true"
      engprod-dr:
        disable: true

  jobwriter:
    source:
      repoURL: https://github.com/barney-ci/aeris
      path: helm/aeris-jobwriter

    # clusterSource extends "source" above with per-cluster parameters.
    clusterSource:
      infra:
        targetRevision: 05f842cbb365c48a71292c895e3bb6065aed7539
      engprod-dr:
        targetRevision: 2eae18d164c2a1307703710190986fb655ce0995

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.  (Cargo culting this).
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-x86_64

    syncPolicy:
      automated: {}

    clusters:
      infra:
        replicaCount: 4
        secrets:
          mount: bsy-aeris-mounted-secrets
        pod:
          addEnv:
          - name: AERIS_SESSION_TOKEN_PATH
            value: /etc/secrets/aeris-session-token
          addArgs:
          - --filter-instance=barney-bsy-*
          - --filter-instance=a2n-ubu2
          - --topic=barney-top-level-jobs
          - --topic=barney-job-metadata
        podAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "3000"
          prometheus.io/scrape: "true"

          profiles.grafana.com/cpu.scrape: "true"
          profiles.grafana.com/cpu.port: "3000"
          profiles.grafana.com/memory.scrape: "true"
          profiles.grafana.com/memory.port: "3000"
          profiles.grafana.com/goroutine.scrape: "true"
          profiles.grafana.com/goroutine.port: "3000"

        service:
          annotations:
            getambassador.io/config: |
              apiVersion: getambassador.io/v2
              kind: Mapping
              name: barney-jobwriter_direct_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-jobwriter.infra.corp.arista.io
              timeout_ms: 36000000
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-jobwriter.barney:3000
      engprod-dr:
        image:
          repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/aeris--jobwriter
        replicaCount: 1
        secrets:
          mount: bsy-aeris-mounted-secrets
        pod:
          addEnv:
          - name: AERIS_SESSION_TOKEN_PATH
            value: /etc/secrets/aeris-session-token
          addArgs:
          - --filter-instance=barney-bsy-0
          - --filter-instance=barney-bsy-1
          - --topic=barney-top-level-jobs
        kafka:
          brokerURL: barney-kafka-cluster-kafka-bootstrap.barney:9092

  jobwriter-head:
    source:
      repoURL: https://github.com/barney-ci/aeris
      path: helm/aeris-jobwriter
      targetRevision: HEAD

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.  (Cargo culting this).
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-x86_64

    syncPolicy:
      automated: {}

    clusters:
      infra:
        replicaCount: 1
        secrets:
          mount: bsy-aeris-mounted-secrets
        pod:
          addEnv:
          - name: AERIS_SESSION_TOKEN_PATH
            value: /etc/secrets/aeris-session-token
          addArgs:
          - --filter-instance=barney-bsy-*
          - --filter-instance=a2n-ubu2
          - --topic=barney-top-level-jobs
          - --topic=barney-job-metadata
        podAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "3000"
          prometheus.io/scrape: "true"

          profiles.grafana.com/cpu.scrape: "true"
          profiles.grafana.com/cpu.port: "3000"
          profiles.grafana.com/memory.scrape: "true"
          profiles.grafana.com/memory.port: "3000"
          profiles.grafana.com/goroutine.scrape: "true"
          profiles.grafana.com/goroutine.port: "3000"

        service:
          annotations:
            getambassador.io/config: |
              apiVersion: getambassador.io/v2
              kind: Mapping
              name: barney-jobwriter-head_direct_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-jobwriter-head.infra.corp.arista.io
              timeout_ms: 36000000
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-jobwriter-head.barney.svc.cluster.local.:3000
        kafka:
          # jobwriter-head points to dev instance of kafka, since it's job is
          # to collect messages written by bsy-head instances.
          brokerURL: 10.243.88.151:30555


  jobstore:
    # jobstore implements the same interface as jobcompat but using aeris
    # as the storage engine.
    source:
      repoURL: https://github.com/barney-ci/aeris
      targetRevision: 583c487200a37e97edce2f9e6bc01eec64f09721
      path: helm/aeris-jobstore

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}

    syncPolicy:
      automated: {}

    clusters:
      infra:
        secrets:
          mount: bsy-aeris-mounted-secrets
        pod:
          addEnv:
          - name: AERIS_SESSION_TOKEN_PATH
            value: /etc/secrets/aeris-session-token
        service:
          annotations:
            prometheus.io/path: /metrics
            prometheus.io/port: "3000"
            prometheus.io/scrape: "true"
            getambassador.io/config: |
              apiVersion: getambassador.io/v2
              kind: Mapping
              name: barney-jobstore_direct_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-jobstore.infra.corp.arista.io
              timeout_ms: 36000000
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-jobstore.barney:3000
      engprod-dr:
        disable: true

  jobstore-head:
    # jobstore implements the same interface as jobcompat but using aeris
    # as the storage engine.
    source:
      repoURL: https://github.com/barney-ci/aeris
      targetRevision: HEAD
      path: helm/aeris-jobstore

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}

    syncPolicy:
      automated: {}

    clusters:
      infra:
        secrets:
          mount: bsy-aeris-mounted-secrets
        pod:
          addEnv:
          - name: AERIS_SESSION_TOKEN_PATH
            value: /etc/secrets/aeris-session-token
        service:
          annotations:
            prometheus.io/path: /metrics
            prometheus.io/port: "3000"
            prometheus.io/scrape: "true"
            getambassador.io/config: |
              apiVersion: getambassador.io/v2
              kind: Mapping
              name: barney-jobstore-head_direct_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-jobstore-head.infra.corp.arista.io
              timeout_ms: 36000000
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-jobstore-head.barney.svc.cluster.local.:3000
      engprod-dr:
        disable: true
      kafka:
        brokerURL: 10.243.88.151:30555 #dev kafka cluster

  sqlviz: &sqlviz
    source:
      repoURL: https://github.com/barney-ci/sqlviz
      targetRevision: f2cef9bf51fba34d7fa92ff38fe288f3da3cb498
      path: helm/sqlviz

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--arch-x86_64

    syncPolicy:
      automated: {}

    clusters:
      infra:
        replicaCount: 8
        pod:
          addEnv:
          - name: "POSTGRES_USER"
            valueFrom:
              secretKeyRef:
                name: jobstats-owner-user.barney-postgres-cluster.credentials.postgresql.acid.zalan.do
                key: username
          - name: "POSTGRES_PASSWORD"
            valueFrom:
              secretKeyRef:
                name: jobstats-owner-user.barney-postgres-cluster.credentials.postgresql.acid.zalan.do
                key: password
          - name: "POSTGRES_NAME"
            value: "jobstats"
          - name: "POSTGRES_HOST"
            value: "barney-postgres-cluster.barney"
          - name: "POSTGRES_PORT"
            value: "5432"
          addArgs:
          resources:
            limits:
              cpu: "3"
              memory: "1Gi"
            requests:
              cpu: "1"
              memory: "500Mi"
        podAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "3000"
          prometheus.io/scrape: "true"

          profiles.grafana.com/cpu.scrape: "true"
          profiles.grafana.com/cpu.port: "3000"
          profiles.grafana.com/memory.scrape: "true"
          profiles.grafana.com/memory.port: "3000"
          profiles.grafana.com/goroutine.scrape: "true"
          profiles.grafana.com/goroutine.port: "3000"

        service:
          annotations:
      engprod-dr:
        disable: true

  sqlviz-head:
    <<: *sqlviz
    source:
      repoURL: https://github.com/barney-ci/sqlviz
      targetRevision: HEAD
      path: helm/sqlviz

    clusters:
      infra:
        replicaCount: 2
        pod:
          addEnv:
          - name: "POSTGRES_USER"
            valueFrom:
              secretKeyRef:
                name: jobstatshead-owner-user.barney-postgres-cluster-head.credentials.postgresql.acid.zalan.do
                key: username
          - name: "POSTGRES_PASSWORD"
            valueFrom:
              secretKeyRef:
                name: jobstatshead-owner-user.barney-postgres-cluster-head.credentials.postgresql.acid.zalan.do
                key: password
          - name: "POSTGRES_NAME"
            value: "jobstatshead"
          - name: "POSTGRES_HOST"
            value: "barney-postgres-cluster-head.barney"
          - name: "POSTGRES_PORT"
            value: "5432"
          - name: "KAFKA_BROKER"
            value: "barney-kafka-cluster-kafka-bootstrap.infra.corp.arista.io:9094"
          addArgs:
          - --filter-instance=barney-bsy-head-*
          resources:
            limits:
              cpu: "3"
              memory: "1Gi"
            requests:
              cpu: "1"
              memory: "500Mi"
        consumerGroup: jobviz-head
      engprod-dr:
        disable: true

  bsy: &bsy_deploy
    source:
      repoURL: https://github.com/barney-ci/barney
      targetRevision: HEAD
      path: helm/bsy

    # clusterSource extends "source" above with per-cluster parameters.
    clusterSource:
      engprod-dr:
        targetRevision: cfb7ad8e4aec0de23aeaeb4941d5696edf28990f

    syncPolicy:
      automated: {}

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--arch-x86_64

    values: &bsy_defaults
      image:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/barney
      hostSettings:
        alpine:
          url: http://mirrors.aristanetworks.com/pub/alpinelinux
        docker:
          # comma separated registry mirrors for docker.io
          registry-mirrors: https://registry-mirror.infra.corp.arista.io/
        secrets:
          bbm_auth_client_id: "" # leave this empty in order for b5 to pick up B5_HOST_SETTINGS_SECRETS_BBM_AUTH_CLIENT_ID.
          bbm_auth_client_secret: "" # leave this empty in order for b5 to pick up B5_HOST_SETTINGS_SECRETS_BBM_AUTH_CLIENT_SECRET.
      secrets:
        gitCredentials: bsy-git-credentials
        environment: bsy-secrets
      influxDBAddr: planck.sjc.aristanetworks.com:8107

      snapshotBuilder:
        parameters:
          - parameter: fallback-quota-num-cpu
            value: 1.0
          - parameter: fallback-quota-memory-bytes
            # 10GB measured on 13. May via spacetime metrics.
            value: "10000000000"
          - parameter: log-head-bytes-max
            value: "3145728"
          - parameter: log-tail-bytes-max
            value: "1085764"
      scheduler:
        maxGoroutines: 100000
        quota:
          - parameter: max-snapshot-downloads
            value: 50
          - parameter: memory-overcommit-ratio
            value: 2
          - parameter: memory-host-reservation
            # 35GB matches the 25GB GOMEMLIMIT used to configure
            # GC in the b5-worker daemon plus 10GB of extra allocation
            # for cache-manager and GC.
            value: "35000000000"

      daemon:
        # influxDBAddr here goes away with newer bsy versions
        # but is retained in case of rollback.
        influxDBAddr: planck.sjc.aristanetworks.com:8107
        kafkaHost: barney-kafka-cluster-kafka-brokers.barney
        kafkaTopLevelJobsTopic: barney-top-level-jobs
        kafkaJobsDetailsTopic: barney-job-details
        maxRunningJobs: 40
        maxAdmittedJobs: 1000
        network:
          firewall:
            # To disable firewall set socketpath: ""
            socketpath: "/sock/b5fwipc.sock"
            loglevel: "debug"
            mode: "permissive"
            allow:
            - barney-api-head.barney.svc.cluster.local. # for barney.ci/b5c%test-floor
            - horseland-gerrit-b64.cloud-dev.corp.arista.io.
            - distwifi.aristanetworks.com.
            - gitlab.aristanetworks.com.
            - distcvp.sjc.aristanetworks.com.
            - wifi-build.sjc.aristanetworks.com.
            - dist.sjc.aristanetworks.com.
            - dist.aristanetworks.com.
            - buildpack.sjc.aristanetworks.com.
            - devopsstats.sjc.aristanetworks.com.
            - artifactory.dev.corp.arista.io.
            - artifactory.infra.corp.arista.io.
            - buildpack.sjc.aristanetworks.com.
            - mirrors.aristanetworks.com.
            - aeristestdb.aristanetworks.com.
            - planck.sjc.aristanetworks.com.
            - 10.243.88.23/32  # default IP for *.infra.corp.arista.io, a VIP on the cluster-external load-balancers
            - 10.0.0.2/32      # non-existent address used in tests code.arista.io/cv/arista%build/turbine/turbine-dependencies-tests-template
            - 10.224.0.0/11    # bs*.sjc.aristanetworks.com. a lot of servers from 10.224.0.4 to 10.247.88.9
            - 0.0.0.0/0
            deny:
            - 10.0.0.0/8
            - 172.16.0.0/12
            - 192.168.0.0/16
          dns:
            enabled: true
            healthCheckFqdn: "test.local."
            zone: |
              test.local. 10 IN A 1.1.1.1
            # 6 search domains are kept for backward compatibility
            # https://discourse.arista.com/t/bsy-deployment-for-the-week-of-4-november-2024/26974/8
            # resolv.conf search settings should be removed when all builds switch to using FQDN.
            resolvConf:
              search:
              - barney.svc.cluster.local
              - svc.cluster.local
              - cluster.local
              - aristanetworks.com
              - sjc.aristanetworks.com
              - syd.aristanetworks.com
              options:
              - "ndots:5"

        clusters:
          # Alternate values:
          # "local" will only attepmt local builds with no attempt to
          # look up the snapshot in cache.
          # "local-with-cache" will attempt a local build and a remote
          # cache lookup similtaneously.
          default: "local-with-lcm"
          remoteCache:
            url: http://barney-api-internal-cache/snapshot

        allowedMounts:
        # NOTE: the builtin exemption is for read-only bind mounts of these files,
        # however some users bind-mount these read-write.
        # It's fine to allow them because the spacetime users do not have write
        # access to the host files.
        - source: '^/etc/(hosts|resolv.conf)$'
          options: [ '^bind$' ]
        - source: '^/sys/fs/cgroup$'
          options: [ '^rbind$' ]

      service: &bsy_annotations
        annotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8181"
          prometheus.io/scrape: "true"
        gcAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8182"
          prometheus.io/scrape: "true"
        firewallAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8183"
          prometheus.io/scrape: "true"
      pod:
        annotations:
          profiles.grafana.com/memory.scrape: "true"
          profiles.grafana.com/memory.port: "8181"
          profiles.grafana.com/cpu.scrape: "true"
          profiles.grafana.com/cpu.port: "8181"
          profiles.grafana.com/goroutine.scrape: "true"
          profiles.grafana.com/goroutine.port: "8181"

    clusters:
      infra: &bsy_infra
        replicas: &replicas 34
        domain: infra.corp.arista.io

        # Makes this deployment visible to the API server
        labels:
          beta.kubernetes.io/arch: amd64
          environment: production

        nodeSelector:
          node-role.kubernetes.io/barney: barney

        service:
          annotations:
            prometheus.io/path: /metrics
            prometheus.io/port: "8181"
            prometheus.io/scrape: "true"

        pod:
          addEnv:
            - name: "GOMEMLIMIT"
              value: "25000000000" # 25GB

        daemon:
          collectorURL: http://barney-job-compat.barney:3000

        # The update strategy ensures that whenever we deploy a new version,
        # we progressively upgrade bsy pods with the new version.
        # To make progress on a deployment, patch the stateful set and reduce
        # the partition ordinal.
        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            partition: 0 # BSY_ROLLOUT barney-bsy-infra

        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule

        cache:
          nodePath: /data2/b5cache
          thresholds:
            # Both the hard and the soft thresholds should be
            # higher than the GC target threshold of 75%.
            hard: 85%
            soft: 80%

        cacheManager:
          nodePort: 20001

      engprod-dr:
        replicas: 2
        domain: engprod-dr.corp.arista.io
        image:
          repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/barney

        # Makes this deployment visible to the API server
        labels:
          beta.kubernetes.io/arch: amd64
          environment: production

        service:
          annotations:
            getambassador.io/config: |
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-bsy_direct_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-bsy.engprod-dr.corp.arista.io
              timeout_ms: 36000000
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-bsy.barney.svc.cluster.local.:8181

        daemon:
          collectorURL: http://barney-job-compat.barney:3000
          bst:
            cgroupDriver: none
          clusters:
            default: "local-with-cache"

        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            partition: 3 # BSY_ROLLOUT barney-bsy-engprod-dr

        cache:
          storageClass: standard-rwo
          size: 200Gi

        cacheManager:
          asContainer: true
          nodePort: 20001
          dnsName: barney-cache-blue

        node:
          nodeConfigPath: "/etc/bsy_node_config"

  # This is a temporary deployment for aarch64. This will eventually be folded into bsy itself,
  # but first, the following issues need to be addressed:
  #
  # 1. [x] barney must be able to build itself on arm64.
  # 2  [ ] a job coordinator that understands architecture must be deployed.
  # 3. [x] barney-docker must be able to forward the architecture to the api server.
  #
  # This bsy is using a fixed image that is currently manually built.
  bsy-aarch64:
    <<: *bsy_deploy

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--arch-aarch64

    values:
      <<: *bsy_defaults

    clusters:
      infra:
        <<: *bsy_infra
        # Temporary reduced from 5 to 4 due to the outage of barney2212.sjc.aristanetworks.com
        # https://discourse.arista.com/t/bsy-deployments-for-the-week-of-16-dec-2024/28473/7
        replicas: 4
        domain: infra.corp.arista.io

        labels:
          beta.kubernetes.io/arch: arm64
          environment: production

        nodeSelector:
          node-role.kubernetes.io/barney-arm64: barney-arm64

        daemon:
          clusters:
            default: "local-with-lcm"

        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            partition: 4 # BSY_ROLLOUT barney-aarch64-infra

        cache:
          nodePath: /data2/b5cache
          name: barney-bsy

      engprod-dr:
        disable: true

  bsy-head-largecache: &bsy_head_application
    source:
      repoURL: https://github.com/barney-ci/barney
      targetRevision: &bsy_head_rev HEAD
      path: helm/bsy

    syncPolicy:
      automated:
        prune: true

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-x86_64

    values:
      <<: *bsy_defaults

      image:
        # The largecache instance of barney runs with `-race` enabled, which gives us extra
        # debugging output which helps find data races (and eventually other kinds of
        # undefined runtime behavior).
        repository: barney-docker.infra.corp.arista.io/barney.ci/barney--debug

      daemon: &bsy_head_daemon
        network:
          firewall:
            socketpath: "/sock/b5fwipc.sock"
            loglevel: "debug"
            mode: "permissive"
            allow:
            - barney-api-head.barney.svc.cluster.local. # for barney.ci/b5c%test-floor
            - "*.corp.arista.io."
            - "*.aristanetworks.com."
            - 10.243.88.23/32  # default IP for *.infra.corp.arista.io, a VIP on the cluster-external load-balancers
            - 10.0.0.2/32      # non-existent address used in tests code.arista.io/cv/arista%build/turbine/turbine-dependencies-tests-template
            - 10.224.0.0/11    # bs*.sjc.aristanetworks.com. a lot of servers from 10.224.0.4 to 10.247.88.9
            - 0.0.0.0/0
            deny:
            - 10.0.0.0/8
            - 172.16.0.0/12
            - 192.168.0.0/16
          dns:
            enabled: true
            timeout: "2s"
            healthCheckFqdn: "test.local."
            zone: |
              test.local. 10 IN A 1.1.1.1
            resolvConf:
              search:
              - aristanetworks.com
              - sjc.aristanetworks.com
              options:
              - "ndots:1"

        allowedMounts:
        - source: '^/sys/fs/cgroup$'
          options: [ '^rbind$' ]

      service:
        <<: *bsy_annotations
        shadow: false
        firewallAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8183"
          prometheus.io/scrape: "true"

      signingAgent: &bsy_head_signing_agent
        address: "https://barneydev-baremetal-bswa-head.dev.corp.arista.io"

      pod:
        annotations:
          profiles.grafana.com/memory.scrape: "true"
          profiles.grafana.com/memory.port: "8181"
          profiles.grafana.com/cpu.scrape: "true"
          profiles.grafana.com/cpu.port: "8181"
          profiles.grafana.com/goroutine.scrape: "true"
          profiles.grafana.com/goroutine.port: "8181"

    clusters:
      infra: &bsy_head_infra
        <<: *bsy_infra
        replicas: 1

        labels:
          beta.kubernetes.io/arch: amd64
          # true makes this instance part of the barney-bsy-head service
          head-like: "true"
          # "production": makes bsy-head part of the production service.
          # "experimental": developer has to select bsy-head manually, e.g., using X-BSY-URL in API server requests.
          environment: experimental

        nodeSelector:
          node-role.kubernetes.io/barney-test: barney-test
        pod:
          allowMultiTenancy: true
          addEnv:
            - name: "GOMEMLIMIT"
              value: "2000000000" # 2GB

        snapshotBuilder:
          parameters:
            - parameter: fallback-quota-num-cpu
              value: 1.0
            - parameter: fallback-quota-memory-bytes
              value: "10000000000" # 10GB
        scheduler:
          quota:
            - parameter: max-snapshot-downloads
              value: 10
            - parameter: memory-overcommit-ratio
              value: 2
            - parameter: memory-host-reservation
              value: "10000000000" # 10GB

        daemon: &bsy_head_daemon_values
          kafkaHost: barney-kafka-cluster-kafka-brokers.barney
          kafkaTopLevelJobsTopic: barney-top-level-jobs
          kafkaJobsDetailsTopic: barney-job-details
          influxDBAddr: planck.sjc.aristanetworks.com:8107
          maxRunningJobs: 20
          maxAdmittedJobs: 1000
          clusters:
            default: "local-with-lcm"
            remoteCache:
              url: http://barney-api/snapshot
          selectorLabels:
            beta.kubernetes.io/arch: amd64
            head-like: "true"

        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            partition: 0 # BSY_ROLLOUT barney-bsy-head-largecache-infra

        cacheManager:
          nodePort: 20001

        cache:
          nodePath: /data2/b5cache
          name: barney-bsy-head
          thresholds:
            # Both the hard and the soft thresholds must be
            # higher than the GC threshold of 75%.
            hard: 85%
            soft: 80%

      engprod-dr:
        disable: true

  bsy-head-smallcache:
    <<: *bsy_head_application

    source:
      repoURL: https://github.com/barney-ci/barney
      targetRevision: *bsy_head_rev
      path: helm/bsy

    values:
      <<: *bsy_defaults

      image:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/barney

      cache:
        thresholds:
          hard: 60GB
          soft: 40GB

      daemon:
        <<: *bsy_head_daemon
      signingAgent: 
        <<: *bsy_head_signing_agent

    clusters:
      infra:
        <<: *bsy_head_infra

        daemon:
          <<: *bsy_head_daemon_values
          clusters:
            default: "local-with-lcm"
            remoteCache:
              url: http://barney-bsy-head-largecache:8181/snapshot

        cache:
          nodePath: /data2/b5cache/smallcache

        cacheManager:
          nodePort: 20002

  bsy-head-aarch64:
    <<: *bsy_head_application

    source:
      repoURL: https://github.com/barney-ci/barney
      targetRevision: *bsy_head_rev
      path: helm/bsy

    values:
      <<: *bsy_defaults

      scheduler:
        maxGoroutines: 100000
        # NOTE: The format for quota has changed, reference
        # bsy_defaults for the new quota format.
        quota:
          - resource: max-snapshot-downloads
            max: 50

      image:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/barney

      daemon:
        <<: *bsy_head_daemon

      pod:
        annotations:
          profiles.grafana.com/cpu.scrape: "true"
          profiles.grafana.com/cpu.port: "8181"
          profiles.grafana.com/memory.scrape: "true"
          profiles.grafana.com/memory.port: "8181"
          profiles.grafana.com/goroutine.scrape: "true"
          profiles.grafana.com/goroutine.port: "8181"

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-aarch64

    clusters:
      infra:
        <<: *bsy_head_infra
        replicas: 1

        labels:
          beta.kubernetes.io/arch: arm64
          # true makes this instance part of the barney-bsy-head service
          head-like: "true"
          # "production": makes bsy-head part of the production service.
          # "experimental": developer has to select bsy-head manually, e.g., using X-BSY-URL in API server requests.
          environment: experimental

        daemon:
          <<: *bsy_head_daemon_values
          selectorLabels:
            beta.kubernetes.io/arch: arm64
            head-like: "true"
          clusters:
            default: "local-with-lcm"
            remoteCache:
              url: http://barney-api/snapshot

        nodeSelector:
          node-role.kubernetes.io/barney-test-arm64: barney-test-arm64

      engprod-dr:
        disable: true

  bsy-experiments:
    source:
      repoURL: https://github.com/barney-ci/barney
      targetRevision: HEAD
      path: helm/bsy

    syncPolicy:
      automated: {}

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-x86_64


    values:
      <<: *bsy_defaults

      image:
        # The largecache instance of barney runs with `-race` enabled, which gives us extra
        # debugging output which helps find data races (and eventually other kinds of
        # undefined runtime behavior).
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/barney--debug

      daemon:
        network:
          firewall:
            socketpath: "/sock/b5fwipc.sock"
            loglevel: "debug"
            mode: "permissive"
            allow:
            - 0.0.0.0/0

      service:
        <<: *bsy_annotations
        shadow: false
        firewallAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8183"
          prometheus.io/scrape: "true"


      pod:
        annotations:
          profiles.grafana.com/cpu.scrape: "true"
          profiles.grafana.com/cpu.port: "8081"
          profiles.grafana.com/memory.scrape: "true"
          profiles.grafana.com/memory.port: "8081"
          profiles.grafana.com/goroutine.scrape: "true"
          profiles.grafana.com/goroutine.port: "8081"
    clusters:
      infra:
        <<: *bsy_infra
        replicas: 2

        labels:
          beta.kubernetes.io/arch: amd64
          # true makes this instance part of the barney-bsy-head service
          # We do not want this instance hangling bsy-head ping load because
          # it does not have the necessary credentials to fetch sources.
          # head-like: "true"
          # "production": makes bsy-head part of the production service.
          # "experimental": developer has to select bsy-head manually, e.g., using X-BSY-URL in API server requests.
          environment: experiments

        nodeSelector:
          node-role.kubernetes.io/barney-experiments: barney-experiments
        pod:
          allowMultiTenancy: true
          addEnv:
            - name: "GOMEMLIMIT"
              value: "2000000000" # 2GB

        snapshotBuilder:
          parameters:
            - parameter: fallback-quota-num-cpu
              value: 1.0
            - parameter: fallback-quota-memory-bytes
              value: "10000000000" # 10GB
        scheduler:
          quota:
            - parameter: max-snapshot-downloads
              value: 10
            - parameter: memory-overcommit-ratio
              value: 2
            - parameter: memory-host-reservation
              value: "10000000000" # 10GB

        daemon:
          kafkaHost: barney-kafka-cluster-kafka-brokers.barney
          kafkaTopLevelJobsTopic: barney-top-level-jobs
          kafkaJobsDetailsTopic: barney-job-details
          influxDBAddr: planck.sjc.aristanetworks.com:8107
          maxRunningJobs: 20
          maxAdmittedJobs: 1000
          clusters:
            default: "local-with-cache"
            remoteCache:
              url: http://barney-api/snapshot
          selectorLabels:
            beta.kubernetes.io/arch: amd64
            head-like: "true"

        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            partition: 2

      engprod-dr:
        disable: true

  repometadata-scraper:
    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/repometadata
      targetRevision: HEAD
      path: helm/repometadata-scraper

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--arch-x86_64

    syncPolicy:
      automated:
        prune: true

    values:
      resources:
        limits:
          cpu: 10
          memory: 30Gi
        requests:
          cpu: 5
          memory: 15Gi
      annotations:
        pyroscope.io/scrape: 'true'
        pyroscope.io/application-name: 'repometadata-scrape'
        pyroscope.io/spy-name: 'gospy'
        pyroscope.io/profile-cpu-enabled: 'true'
        pyroscope.io/profile-mem-enabled: 'true'
        pyroscope.io/profile-goroutines-enabled: 'true'
        pyroscope.io/port: '6060'

      secrets:
      - repometadata-firestore-credentials
      - repometadata-gerrit-key
      - repometadata-gerrit-username
      - repometadata-gerrit-api-password
      - barney-ci-app

      scrape_config:
        output-url:
          host: "http://repometadata.barney:8080"
        # Gitarband-gerrit takes much longer to scrape than the
        # rest of the targets due to its size. It would be nice
        # to have a per-target timeout but this requires a lot of
        # refactoring in the scraper.
        scrape-target-timeout: 90m
        scrape-repo-timeout: 20m
        gerrit-targets:
        - anon-url: https://horseland-gerrit.infra.corp.arista.io
          auth-url: ssh://horseland-gerrit.infra.corp.arista.io:29430
          default-repo-prefix: arista.com/infra/barney
          name: horseland gerrit
          gitauth:
            ignore-host-keys: true
            ssh-key:
              file: /etc/secrets/repometadata-gerrit-key/repometadata-gerrit-key
            username:
              file: /etc/secrets/repometadata-gerrit-username/repometadata-gerrit-username

        - name: gitarband gerrit
          anon-url: https://gitarband-gerrit.infra.corp.arista.io
          auth-url: ssh://gitarband-gerrit.infra.corp.arista.io:29400
          default-repo-prefix: arista.com
          parallelism: 10
          gitauth:
            ignore-host-keys: true
            ssh-key:
              file: /etc/secrets/repometadata-gerrit-key/repometadata-gerrit-key
            username:
              file: /etc/secrets/repometadata-gerrit-username/repometadata-gerrit-username

        - anon-url: https://gerrit.corp.arista.io
          auth-url: ssh://gerrit.corp.arista.io:29418
          name: baremetal gerrit
          project-to-reponame:
            ArCloud: github.com/aristanetworks/ArCloud
            arista: arista
            synce: arista.com/synce
          gitauth:
            ignore-host-keys: true
            ssh-key:
              file: /etc/secrets/repometadata-gerrit-key/repometadata-gerrit-key
            username:
              file: /etc/secrets/repometadata-gerrit-username/repometadata-gerrit-username
          apiauth:
            password:
              file: /etc/secrets/repometadata-gerrit-api-password/repometadata-gerrit-api-password
            username:
              file: /etc/secrets/repometadata-gerrit-username/repometadata-gerrit-username

        github-targets:
        - name: "barney-ci organization"
          org: barney-ci
          # keep the parallelism at 1 because otherwise we run into github rate limiting
          parallelism: &parallelism 2
          auth:
            github: &rpmd-github-authspec
              private-key:
                file: "/etc/secrets/barney-ci-app/private_key"
              app-id:
                value: "237227"
              installation-id:
                value: "29145537"
        - name: "aristanetworks organization"
          org: aristanetworks
          parallelism: *parallelism
          auth:
            github:
              <<: *rpmd-github-authspec
              installation-id:
                value: "29693060"
        - name: "untangle organization"
          org: untangle
          parallelism: *parallelism
          auth:
            github:
              <<: *rpmd-github-authspec
              installation-id:
                value: "31999231"
        - name: "arista-eos-external organization"
          org: arista-eos-external
          parallelism: *parallelism
          auth:
            github:
              <<: *rpmd-github-authspec
              installation-id:
                value: "35591729"
        - name: "bigswitch organization"
          org: bigswitch
          parallelism: *parallelism
          auth:
            github:
              <<: *rpmd-github-authspec
              installation-id:
                value: "41233582"
        - name: "arista-platform-mfg organization"
          org: arista-platform-mfg
          auth:
            github:
              <<: *rpmd-github-authspec
              installation-id:
                value: "46507445"
        - name: "arista-ntk organization"
          org: arista-ntk
          auth:
            github:
              <<: *rpmd-github-authspec
              installation-id:
                value: "47124656"

    clusters:
      infra: {}
      engprod-dr:
        disable: true


  repometadata:
    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/repometadata
      targetRevision: HEAD
      path: helm/repometadata

    valueFiles:
      - ../../deployment/values.yaml

    clusters:
      infra:
        disable: true
        image:
          tag: f35950a0903552ae3ec7d3a54e0fb48a3b5a43d9
        service:
          domain: infra.corp.arista.io
      engprod-dr:
        disable: true
        image:
          repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/repometadata
          pullPolicy: IfNotPresent
          tag: f35950a0903552ae3ec7d3a54e0fb48a3b5a43d9
        service:
          domain: infra.corp.arista.io

  topicmanager:

    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/topicmanager
      targetRevision: HEAD
      path: helm/btm

    valueFiles:
      - ../../deployment/btm-topic-finders.yaml
      - ../../deployment/btm-values.yaml

    values:
      service:
        shadow: false
        weight: ""
      image:
        repository: docker.corp.arista.io/btm
        tag: ""
      resources:
        limits:
          cpu: "2"
          memory: "1Gi"
        requests:
          cpu: "1"
          memory: "500Mi"

    clusters:
      infra:
        disable: true
        service:
          domain: infra.corp.arista.io
        firestore:
          project: sw-infra-barney-infra
        image:
          tag: v0.6.1-27-gdac1643
      engprod-dr:
        disable: true
        image:
          repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/btm
          tag: v0.6.1-27-gdac1643
        service:
          domain: infra.corp.arista.io
        firestore:
          project: sw-infra-barney-infra

  grl-red: &grl_red

    source:
      repoURL: https://github.com/barney-ci/grl
      targetRevision: HEAD
      path: helm/grl

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}

    values:

      image:
          repository: barney-docker.infra.corp.arista.io/barney.ci/grl

      podAnnotations:
        maintainer: barney-dev
        prometheus.io/scrape: "true"
        prometheus.io/port: "6060"

      config:
        auth:
          method: "authspec-from-regex"
          configpath: "/etc/grl.yaml"

        authspec-from-re: &github-authspecs
          - repo-re: "http.*github.com/.*"
            authspec:
              github: &github-authspec
                private-key:
                  file: "/etc/secrets/barney-ci-app/private_key"
                app-id:
                  value: 237227
                default-installation-id:
                  value: 29145537

      secrets:
        - barney-ci-app

      redis:
        host: "redis-grl.barney"

    clusters:
      infra:
        service:
          domain: infra.corp.arista.io
      engprod-dr:
        disable: true

  grl-blue:
    <<: *grl_red

  bar-test-prod:

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}

    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/bar-test/driver
      targetRevision: 87e1eaafbd2a9703afb0df37d7a0146d4d5ed893
      path: helm/bar-test

    syncPolicy:
      automated: {}

    clusters:
      infra:
        replicaCount: 1
        # Period specifies how often bar-test will start a cycle; where
        # a cycle includes committing once to each of the 4 ghost repos
        # and watching that commit roll to consumers.
        deployment:
          period: 2h
          bsyHost: bsy.infra.corp.arista.io
          grlHost: redis-grl.barney
          influxAddr: planck.sjc.aristanetworks.com:8107
          gitAuthSecretName: bar-test-horseland-gerrit-secrets

        podAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "6062"
          prometheus.io/scrape: "true"

      engprod-dr:
        disable: true

  multibar-alpha: &multibar_alpha
    # The alpha instance of multibar only handles robot traffic,
    # specifically barney.ci/bar-test/{inky,blinky,pinky,clyde}.

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-x86_64

    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/bar
      targetRevision: HEAD
      path: helm/multibar

    namespace: barney-bar

    syncPolicy:
      automated:
        prune: true

    values: &multibar_alpha_values
      image:
        repository: barney-docker.infra.corp.arista.io/barney.ci/bar--multibar
      config: &multibar_alpha_values_config
        apiserver:
          url: "https://barney-api.infra.corp.arista.io"
          job-labels:
            multibar-instance: "alpha"
            multibar-variant: "prober"
        bsy:
          url: "https://bsy.infra.corp.arista.io"
          job-watcher: kafka
          kafka-watcher:
            kafka:
              brokers:
                - barney-kafka-cluster-kafka-bootstrap.barney:9092
              groupid: multibar-alpha
              grouptopics:
                - barney-top-level-jobs
                # This instance doesn't actually care about any of the jobs on this
                # topic, but following it in the alpha deployment allows us to monitor
                # whether the writer (barnzilla) is maintaining compatability with
                # the reader (BAR) as BAR updates over time.
                - barney-barnzilla-jobs
              # `-2` is equivalent to kafka.FirstOffset; re-read
              # the entire topic from start to build up a list
              # of probes that may have finished while running
              startoffset: -2
        repo-metadata:
          url: "http://repometadata.barney:8080"
          external-url: "https://repometadata.infra.corp.arista.io"
        grl:
          host: redis-grl.barney
          port: 6379
        influx:
          addr: planck.sjc.aristanetworks.com:8107
        global-fallback-roll-policy-ref: code.arista.io/infra/barney/barney-ops#main
        git-cache-url: https://btm.infra.corp.arista.io

        authspec-from-re:
          - repo-re: "http.*github.com/.*"
            authspec:
              github: *github-authspec
          - repo-re: "http.*"
            authspec:
              email:
                value: "bar@barney.ci"
              file: "/etc/secrets/multibar-alpha-secrets"

      repoFilters:
      - 'x-bar.version == "alpha"'

      repoLabels:
      - 'archived=false'

      secrets: &multibar_alpha_values_secrets
      - name: multibar-alpha-secrets
        subPath: gitauth.yaml
      - name: multibar-firestore-secrets
        subPath: firestore.json
        file: firestore.json
      - name: barney-ci-app
        subPath: null

      podAnnotations:
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/cpu.port: "6061"
        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/memory.port: "6061"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/goroutine.port: "6061"
        profiles.grafana.com/mutex.scrape: "true"
        profiles.grafana.com/mutex.port: "6061"
        profiles.grafana.com/block.scrape: "true"
        profiles.grafana.com/block.port: "6061"

      resources:
        requests:
          cpu: 100m
          memory: 300Mi

    clusters:
      infra:
        addEnv:
        - name: BAR_APISERVER_AUTH_CLIENT_ID
          valueFrom:
            secretKeyRef:
              name: bar-barney-oauth-client-credentials
              key: client_id
        - name: BAR_APISERVER_AUTH_CLIENT_SECRET
          valueFrom:
            secretKeyRef:
              name: bar-barney-oauth-client-credentials
              key: client_secret
        otel:
          enabled: true
        service:
          domain: infra.corp.arista.io
        config: &multibar_alpha_values_config_infra
          firestore:
            project: sw-infra-barney-autoroller
            credentials-file: /etc/secrets/firestore.json
      engprod-dr:
        disable: true

  multibar-alpha-renovate:
    # The alpha instance of multibar with renovate that only handles robot traffic,
    # specifically barney.ci/bar-test/{a}.

    <<: *multibar_alpha

    values:
      <<: *multibar_alpha_values


      config:
        <<: *multibar_alpha_values_config
        apiserver:
          url: "https://barney-api.infra.corp.arista.io"
          job-labels:
            multibar-instance: "alpha"
            multibar-variant: "renovate"
        bsy:
          url: "https://bsy.infra.corp.arista.io"
          job-watcher: kafka
          kafka-watcher:
            kafka:
              brokers:
                - barney-kafka-cluster-kafka-bootstrap.barney:9092
              groupid: multibar-alpha-renovate
              grouptopics:
                - barney-top-level-jobs
              # `-1` is equivalent to kafka.LastOffset; skip
              # over any events that predate the pod because
              # there is no use in monitoring jobs that we
              # didn't start.
              startoffset: -1
        default:
          greedy-roll:
            enabled: false

      multibarExecutable: multibar-renovate

      image:
        repository: barney-docker.infra.corp.arista.io/barney.ci/bar--multibar-renovate

      repoFilters:
      - 'x-bar.version == "alpha"'

      repoLabels:
      - 'archived=false'

  multibar-beta:
    # The beta instance of multibar mostly handles traffic for
    # advanced barney users who are likely to notice if anything
    # strange happens.

    <<: *multibar_alpha

    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/bar
      targetRevision: HEAD
      path: helm/multibar

    values:
      <<: *multibar_alpha_values

      config:
        <<: *multibar_alpha_values_config
        apiserver:
          url: "https://barney-api.infra.corp.arista.io"
          job-labels:
            multibar-instance: "beta"
            multibar-variant: "prober"
        bsy:
          url: "https://bsy.infra.corp.arista.io"
          job-watcher: kafka
          kafka-watcher:
            kafka:
              brokers:
                - barney-kafka-cluster-kafka-bootstrap.barney:9092
              groupid: multibar-beta
              grouptopics:
                - barney-top-level-jobs
              # `-2` is equivalent to kafka.FirstOffset; re-read
              # the entire topic from start to build up a list
              # of probes that may have finished while running
              startoffset: -2

      repoFilters:
      - 'x-bar.version == "beta"'

      repoLabels:
      - 'archived=false'

  multibar-beta-renovate:
    # The beta instance of multibar mostly handles traffic for
    # advanced barney users who are likely to notice if anything
    # strange happens.

    <<: *multibar_alpha

    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/bar
      targetRevision: HEAD
      path: helm/multibar

    values:
      <<: *multibar_alpha_values

      resources:
        requests:
          cpu: 1
          memory: 2Gi

      config:
        <<: *multibar_alpha_values_config
        apiserver:
          url: "https://barney-api.infra.corp.arista.io"
          job-labels:
            multibar-instance: "beta"
            multibar-variant: "renovate"
        bsy:
          url: "https://bsy.infra.corp.arista.io"
          job-watcher: kafka
          kafka-watcher:
            kafka:
              brokers:
                - barney-kafka-cluster-kafka-bootstrap.barney:9092
              groupid: multibar-beta-renovate
              grouptopics:
                - barney-top-level-jobs
              # `-1` is equivalent to kafka.LastOffset; skip
              # over any events that predate the pod because
              # there is no use in monitoring jobs that we
              # didn't start.
              startoffset: -1
        default:
          greedy-roll:
            enabled: false

      multibarExecutable: multibar-renovate

      image:
        repository: barney-docker.infra.corp.arista.io/barney.ci/bar--multibar-renovate

      repoFilters:
      - 'x-bar.version == "beta"'

      repoLabels:
      - 'archived=false'

  multibar-production:
    # The production instance of multibar handles all other traffic.

    <<: *multibar_alpha

    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/bar
      targetRevision: ba418ff13a05bb6ad9fb17d6a31010a80d26c91b
      path: helm/multibar

    values:
      <<: *multibar_alpha_values

      resources:
        requests:
          cpu: 500m
          memory: 4Gi

      config:
        <<: *multibar_alpha_values_config
        apiserver:
          url: "https://barney-api.infra.corp.arista.io"
          job-labels:
            multibar-instance: "production"
            multibar-variant: "prober"
        bsy:
          max-concurrent-jobs: 20
          url: "https://bsy.infra.corp.arista.io"
          job-watcher: kafka
          kafka-watcher:
            kafka:
              brokers:
                - barney-kafka-cluster-kafka-bootstrap.barney:9092
              groupid: multibar-production
              grouptopics:
                - barney-top-level-jobs
              # `-2` is equivalent to kafka.FirstOffset; re-read
              # the entire topic from start to build up a list
              # of probes that may have finished while running
              startoffset: -2

      repoFilters:
      - 'x-bar.version == "production"'

      repoLabels:
      - 'archived=false'

  multibar-production-renovate:
    # The production instance of multibar handles all other traffic.

    <<: *multibar_alpha

    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/bar
      targetRevision: ba418ff13a05bb6ad9fb17d6a31010a80d26c91b
      path: helm/multibar

    values:
      <<: *multibar_alpha_values

      resources:
        requests:
          cpu: 5
          memory: 8Gi

      config:
        <<: *multibar_alpha_values_config
        apiserver:
          url: "https://barney-api.infra.corp.arista.io"
          job-labels:
            multibar-instance: "production"
            multibar-variant: "renovate"
        bsy:
          max-concurrent-jobs: 100
          url: "https://bsy.infra.corp.arista.io"
          job-watcher: kafka
          kafka-watcher:
            kafka:
              brokers:
                - barney-kafka-cluster-kafka-bootstrap.barney:9092
              groupid: multibar-production-renovate
              grouptopics:
                - barney-top-level-jobs
              startoffset: -1
        default:
          greedy-roll:
            enabled: false

      multibarExecutable: multibar-renovate

      image:
        repository: barney-docker.infra.corp.arista.io/barney.ci/bar--multibar-renovate

      repoFilters:
      - 'x-bar.version == "production"'

      repoLabels:
      - 'archived=false'

  multibar-barnzilla-repos:
    # This instance is solely responsible for rolling into the
    # barnzilla-repos repo. The reason we keep this separate is
    # because restarting this instance incurs a much greater cost,
    # as we may accidentally create duplicate MUTs into trunk.

    <<: *multibar_alpha

    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/bar
      targetRevision: ba418ff13a05bb6ad9fb17d6a31010a80d26c91b
      path: helm/multibar

    values:
      <<: *multibar_alpha_values

      resources:
        requests:
          cpu: 300m
          memory: 3Gi

      config:
        <<: *multibar_alpha_values_config
        apiserver:
          url: "https://barney-api.infra.corp.arista.io"
          job-labels:
            multibar-instance: "barnzilla-repos"
            multibar-variant: "prober"
        scheduler:
          # Probes launched by this instance of multibar will be
          # executed by barnzilla rather than bsy.
          probe-arch: barnzilla
        bsy:
          url: "https://bsy.infra.corp.arista.io"

          # This instance is liable to get barnzilla-repos probes running
          # for hours at a time which don't take up much load on bsy pods.
          # Bumping up this max should minimally impact bsy load.
          max-concurrent-jobs: 75

          job-watcher: kafka
          kafka-watcher:
            kafka:
              brokers:
                - barney-kafka-cluster-kafka-bootstrap.barney:9092
              groupid: multibar-barnzilla-repos
              grouptopics:
                - barney-top-level-jobs
                - barney-barnzilla-jobs
              startoffset: -2

      repoFilters:
      - "x-bar.version=barnzilla-repos"

      repoLabels: []

  bar-observer:
    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-x86_64

    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/bar
      targetRevision: ba418ff13a05bb6ad9fb17d6a31010a80d26c91b
      path: helm/observer

    syncPolicy:
      automated: {}

    values: &bar-observer-values
      image:
        repository: barney-docker.infra.corp.arista.io/barney.ci/bar--observer
      secrets:
        - name: barney-ci-app
          subPath: null
        - name: multibar-firestore-secrets
          subPath: firestore.json
          file: firestore.json
      envFromSecrets:
        - bar-observer-environment
      config:
        <<: *multibar_alpha_values_config
        authspec-from-re: *github-authspecs
      nodeSelector:
        node-role.kubernetes.io/barney-aux: barney-aux
      tolerations:
        - key: type
          operator: Equal
          value: barney
          effect: NoSchedule
      securityContext:
        runAsUser: 10000
      otel:
        enabled: true
      podAnnotations:
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/cpu.port: "6062"
        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/memory.port: "6062"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/goroutine.port: "6062"
        profiles.grafana.com/mutex.scrape: "true"
        profiles.grafana.com/mutex.port: "6062"
        profiles.grafana.com/block.scrape: "true"
        profiles.grafana.com/block.port: "6062"

      resources:
        requests:
          cpu: 500m
          memory: 1Gi

    clusters:
      dev:
        disable: true
      infra:
        service:
          annotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind: Mapping
              name: bar-observer-mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: bar-observer.infra.corp.arista.io
              timeout_ms: 36000000
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-bar-observer.barney.svc.cluster.local.:6062
              cors:
                origins:
                  - "*"
        config: &bar_observer_clusters_infra_config
          <<: *multibar_alpha_values_config_infra
          git-cache-url: https://btm.infra.corp.arista.io
          links:
            probe:
              # Only include the job status link if the JobID is defined.
              # It will either have the JobID defined or the ReviewNumber defined.
              status: |-
                {{ if regexpMatch ".+" .JobID -}}https://bsy.infra.corp.arista.io/job/{{- .JobID}}/status{{- end}}
              # Ensure that the following regex matches the default value configured by barnzilla at mut/mutmanager.go#31
              mut: |-
                {{if regexpMatch "bar\\..*" .JobID -}}https://aboard.infra.corp.arista.io/mut/{{- .JobID}}{{- end}}
            relationship:
              grafana: "https://raven.infra.corp.arista.io/d/roll-incoming/incoming-roll-state?orgId=1&var-repo={{.TargetRepo | queryEscape}}&var-source={{.IncomingRepo | queryEscape}}&from=now-7d&to=now"

      engprod-dr:
        disable: true

  bar-observer-head:
    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-x86_64

    source:
      repoURL: https://horseland-gerrit.infra.corp.arista.io/bar
      targetRevision: HEAD
      path: helm/observer

    syncPolicy:
      automated: {}

    values: *bar-observer-values

    clusters:
      dev:
        disable: true
      infra:
        service:
          annotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind: Mapping
              name: bar-observer-head-mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: bar-observer-head.infra.corp.arista.io
              timeout_ms: 36000000
              idle_timeout_ms: 36000000
              prefix: /
              service: barney-bar-observer-head.barney.svc.cluster.local.:6062
              cors:
                origins:
                  - "*"
        config: *bar_observer_clusters_infra_config
        otel:
          enabled: true
      engprod-dr:
        disable: true

  bbm:
    # Barney Bus Manager

    source:
      repoURL: https://github.com/barney-ci/bbm
      targetRevision: 5e56c3051b0e88b0255b7c84fe4ea56a6846de7a
      path: helm/bbm

    syncPolicy:
      automated: {}

    parameters: &bbm-parameters
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}--arch-x86_64
    - name: config.docker-image
      value: barney-docker.infra.corp.arista.io/barney.ci/bbm--bus:${ARGOCD_APP_REVISION}--arch-x86_64
    - name: config.dind-rootless.image
      value: barney-docker.infra.corp.arista.io/barney.ci/bbm--dind-rootless:${ARGOCD_APP_REVISION}--arch-x86_64

    values: &bbm-values
      image:
        repository: barney-docker.infra.corp.arista.io/barney.ci/bbm--bbm
      certSecrets: arista-certs
      fileSecrets: &bbm-file-secrets
        - barney-ci-app
      podAnnotations:
        maintainer: barney-dev
        moniker.spinnaker.io/cluster: bbm
        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/memory.port: "8080"
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/cpu.port: "8080"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/goroutine.port: "8080"
      config: &bbm-values-config
        controller:
          concurrency: 500
        k8s:
          node-selector:
            node-role.kubernetes.io/bus: bus
          tolerations:
            type: bus:NoSchedule
          pod-annotations:
            prometheus.io/scrape: "true"
            prometheus.io/port: &busMetricsPort "9090"
        metrics:
          port: *busMetricsPort
        ldap:
          url: ldaps://itsys-dcr006p.aristanetworks.com
          base-dn: ou=ARISTA,dc=aristanetworks,dc=com
          home-directory-fallback-template: homedir101.sjc.aristanetworks.com:/homedirfs/${uid}
          home-directory-check-url: http://sentry-web-service.sentry/users/${uid}/central-home-directory
          bind-account:
            username: ${BBM_LDAP_BIND_ACCOUNT_USERNAME}
            password: ${BBM_LDAP_BIND_ACCOUNT_PASSWORD}
        btm:
          addr: btm-grpc.infra.corp.arista.io:443
        manager:
          leader-election: true
        api:
          baseurl: https://barney-api.infra.corp.arista.io
        volumeSource:
          persistentVolumeClaim: {}
        persistentVolumeClaim:
          spec:
            accessModes:
            - ReadWriteOnce
            resources:
              requests:
                storage: 100Gi
            storageClassName: local-pv-dirs
        authspec-from-re: *github-authspecs
        bus-file-secrets:
          - ssh-ca
        repometadata:
          url: http://repometadata.barney:8080
        defaultBusResourceProfile: small
        busResources:
          small:
            requests:
              cpu: "0.8"
              memory: "3760Mi"
              storage: 100Gi
            limits:
              cpu: "20"
              memory: "94Gi"
              storage: 100Gi
          medium:
            requests:
              cpu: "1.6"
              memory: "7520Mi"
              storage: 200Gi
            limits:
              cpu: "20"
              memory: "150Gi"
              storage: 200Gi
          large:
            requests:
              cpu: "6.4"
              memory: "30080Mi"
              storage: 800Gi
            limits:
              cpu: "20"
              memory: "250Gi"
              storage: 800Gi
        auth:
          url: https://auth.infra.corp.arista.io
          authentication:
            optional: true
            enable: false
          authorization:
            enable: false
            allowed-domains: ["arista.com"]
            admin-groups: ["k8s-infra-bus-admin@arista.com"]
          client:
            id: ${BBM_AUTH_CLIENT_ID}
            secret: ${BBM_AUTH_CLIENT_SECRET}
      envSecrets:
        - bbm-dex-credentials
        - bbm-ldap-bind-account
      certs:
        AristaIntermediate.crt: |
          -----BEGIN CERTIFICATE-----
          MIIExjCCA66gAwIBAgIQDf2t4p1F9We3hbvngBAZKTANBgkqhkiG9w0BAQsFADBd
          MQswCQYDVQQGEwJVUzEcMBoGA1UEChMTQXJpc3RhIE5ldHdvcmtzIEluYzEwMC4G
          A1UEAxMnQXJpc3RhIE5ldHdvcmtzIEludGVybmFsIENlcnQgQXV0aG9yaXR5MB4X
          DTE2MDkwOTAwMDAwMVoXDTM2MDkwODIzNTk1OVowYjELMAkGA1UEBhMCVVMxHDAa
          BgNVBAoTE0FyaXN0YSBOZXR3b3JrcyBJbmMxNTAzBgNVBAMTLEFyaXN0YSBOZXR3
          b3JrcyBJbnRlcm5hbCBDZXJ0IEF1dGhvcml0eSAtIEEyMIIBIjANBgkqhkiG9w0B
          AQEFAAOCAQ8AMIIBCgKCAQEAza7xG6jzDoLOf+hHg4ailnmI2iunDP0oQLl/aCos
          UtuiyMZPZf8rfBCYkojS4PZefHlA3vEkSW8dkugGh6ztyvt/gBJJudIPFbzQF6aV
          7Ff8yKAMCyqN2XOYRniKOW0mMdPzFfQeT9bM3DBOeuPEZ+W2Sz/tAbP9dpoYBx46
          ZAQpWxgIasyGlJvh+0a92tlsX8PYycrKu1rqYH3HhCSTA7pGukJRXFmgrXZbHA6W
          WFMkYIqUAkN4DAyEYnWH+vs9qaC12G36ajzq6C+uFv3TF0XjM/EyoioLcijkK0GQ
          hMm6+HUnyIjFiDf/xKE6MKowvyxDAxOXuT5ur4nOyguJGQIDAQABo4IBezCCAXcw
          HQYDVR0OBBYEFDHkm4EUfwkTwC+x25PTXDtLQoDSMBIGA1UdEwEB/wQIMAYBAf8C
          AQAwDgYDVR0PAQH/BAQDAgGGMB8GA1UdIwQYMBaAFEFOTjbD3XhTslxf5Nc82shk
          ddutMD0GA1UdIAQ2MDQwMgYEVR0gADAqMCgGCCsGAQUFBwIBFhxodHRwczovL3d3
          dy5kaWdpY2VydC5jb20vQ1BTMDQGCCsGAQUFBwEBBCgwJjAkBggrBgEFBQcwAYYY
          aHR0cDovL29jc3AuZGlnaWNlcnQuY29tMIGbBgNVHR8EgZMwgZAwRqBEoEKGQGh0
          dHA6Ly9jcmwzLmRpZ2ljZXJ0LmNvbS9BcmlzdGFOZXR3b3Jrc0ludGVybmFsQ2Vy
          dEF1dGhvcml0eS5jcmwwRqBEoEKGQGh0dHA6Ly9jcmw0LmRpZ2ljZXJ0LmNvbS9B
          cmlzdGFOZXR3b3Jrc0ludGVybmFsQ2VydEF1dGhvcml0eS5jcmwwDQYJKoZIhvcN
          AQELBQADggEBAAVIr6QUWAa0sdqyOCS8eOKmrDR8ECGWpghiYswshJbmDoyjNmwG
          V0A0mblJyF/qL+3sELBc1Ws1xdRGt++0hp55LustOUyj59nsJlzNou/tNAuh3XTA
          Qf4EYbcVdNDmHfZfTHrEeDwLa0bQRNYxvZaDbpCn5w2MrMOM30DRrLpWiqTRRR8/
          8TWwi6SQ+ZR3SE7csRNhzQ11NV3nkJrXjuP75BC93j8ZyB/VG9HDkwyp3SMV2+Y5
          pA+bkckB1EQLY9Je1yqNxLQo2c2OcMTthItiUY4C8r8qkURAmwTHMkkaYWcvrilC
          oixIaZv9Nh1Xs9jeArZLA5vSF3SV78ivg78=
          -----END CERTIFICATE-----
        aristarootCA.crt: |
          -----BEGIN CERTIFICATE-----
          MIIKNTCCBh2gAwIBAgIJAMataALFvjMbMA0GCSqGSIb3DQEBBQUAMIGwMQswCQYD
          VQQGEwJVUzETMBEGA1UECAwKQ2FsaWZvcm5pYTEUMBIGA1UEBwwLU2FudGEgQ2xh
          cmExHDAaBgNVBAoME0FyaXN0YSBOZXR3b3JrcyBJbmMxKjAoBgNVBAMMIUFyaXN0
          cyBOZXR3b3JrcyBSb290IENBIEF1dGhvcml0eTEsMCoGCSqGSIb3DQEJARYdaXQt
          c3VwcG9ydEBhcmlzdGFuZXR3b3Jrcy5jb20wHhcNMTMwMjIwMDA0MTI3WhcNMzMw
          MjE1MDA0MTI3WjCBsDELMAkGA1UEBhMCVVMxEzARBgNVBAgMCkNhbGlmb3JuaWEx
          FDASBgNVBAcMC1NhbnRhIENsYXJhMRwwGgYDVQQKDBNBcmlzdGEgTmV0d29ya3Mg
          SW5jMSowKAYDVQQDDCFBcmlzdHMgTmV0d29ya3MgUm9vdCBDQSBBdXRob3JpdHkx
          LDAqBgkqhkiG9w0BCQEWHWl0LXN1cHBvcnRAYXJpc3RhbmV0d29ya3MuY29tMIIE
          IjANBgkqhkiG9w0BAQEFAAOCBA8AMIIECgKCBAEAt8nYzeB/2n1KUC1CIZBCPMjj
          jCF2NhoZ2JE+n4UKVWNhrIDz9tVLhpUIbK6BJjE5xunFI4LuvIZjzi6CpCRCgNdx
          hDMQPoXcWKbRk6ov6mWOlKxEJJ0xWmRMO/8jj7QjtlPedvwFVbQQRsjWb0yHSaaI
          nSyc8bysmM+cLeqbTm5EkB57Dml+Y9rw/brgi2HUhsZhGZev4xofsln5quOzvu1N
          irfsJRnNL5U0ORETBpGEw8Um1gHTC0PG881ym0g0XxmNQyGyMjPCaRczEsOMw32M
          hQC/q6r0zCpy3mrJcgiIw74Ba2903rY5vJbqqUnt9uJEYoHFc9dEtti+GWYyx4Zc
          5aNBKBNV0mBhpl5OCwQR436dzlMk4H/4Lvy8ZubjT4oPQ8Cog0HdJjNzCDG/7Nfx
          Bb5zfFd2LUN7zOpZMlNbaUvliR1ocUkGEIIaweV2peTIbf0+FI52dmPgX0goTBKV
          QLEhgJ6euKAIvObgyOJnEFJoRbxVnMwVtPxg6rC2VbCaYkTvXM5crjXGNb5nR+YT
          /jYev8poV2LhZKuby3qWwlDABlqwl7wTI9FXpkUROfGqe14oi6MaS8JfeqzgpA6U
          XPfjXAq9SnS/aIL+AFAs8XmV1T5wu4rKho1VoK0RQhY6abC9coblI7HZuRBHJv93
          xB6fV4JBEAJlMLdh36+UBLGVnp7iQ13Tpuc6vSfvce+h+vOH42pu4NOeofPLqa1s
          prpiFqPpopEkwq914kDJywcy86n/DSUqc6DWY/RUTogPLP9+Bi6UCkrN4yWt7yft
          OYZC4DbNiHJB4WHm/Rb93lGa8lp9qnvLw5Cy1LPf4wu9czcFLegFt8291o6CvCTA
          rtYDxwBdoytvrFG5Epi/DeWocT8XevY0kOkOOWmHfdmlNdcu9xnYWbaTpR5ROQMN
          8v3gJxqc0bOj0gruiuRu/Wk8rkJ5uHUlDGJ97RoJpguT1wQKA0cjqO3mta1J+98m
          hZW4COq87wbq/DO8DUSI+cLjG7mrEOjdlaOODV3VPbLZl7/lsn0iyi3hZEcEdWMg
          ZNd4vHm3C24gDqz5iKnISWtREbR5lVQp2Fffbl0fRLjg3EP0BVVIlCoL62xyL2ql
          L1rydj7VITQJnwiWa2iTQBilfI0IzZ1zlHzY5aDcAjQDjaPBMsCnv6Tc1k3yOvuy
          MLlB8oYBh1VAzmFNWMlIt373tXcsJfLyCIeCgJAk8f+4l+z0Lhp05McOwJ4tfPvD
          uYvI/Bq+Q8rDSd07yWqY4TnAfCuevsvAlEiVy5H1UfpyJSujA0RjZaswMHYcokLI
          wcy+yOxuqTbxmYOUuwAqbpXj5AAN5X5pO3UrDCSMfBlZWrGwnvhJJOjNwuU2dwID
          AQABo1AwTjAdBgNVHQ4EFgQUkH0lMN4N8oBjwZjaEb0NLNOFWjEwHwYDVR0jBBgw
          FoAUkH0lMN4N8oBjwZjaEb0NLNOFWjEwDAYDVR0TBAUwAwEB/zANBgkqhkiG9w0B
          AQUFAAOCBAEANBp6rSSuxsDVDb0STODBQk0ODw1j1isChNMk2PmJRbIondpExwgi
          nVPAbzCP9MFdmqwfDr/hdHcW6sMVb5YUA5C/Y/YUkHJAcnh9OfZ+YhCbqWgjQut+
          f4PEiyhckLFSEiU42tK+e2U47XYpJDDPRFpZP2yoau2ooUzGGRh56g+2sYQkGmIp
          oXJA/6Pgu9NjI1auDkuIswOS8YngCNnRpR3ANA2BBHbwYPNDOJbIr7GH7dzJooE6
          a8XMJiOI55K8DDD0x7OCOtQcOTS7HHKFtVbIMXYI0SEOsd4l9TbPD65vEZWTZ5kK
          bitBrRYm62TgjuHraLiFruAEXK34HIMSiLfbxLa7TdF4aIUJB9dRpCcCt6Vf3GR/
          Cj88wyiqPxZc6+bEPtuY1JPS79xO97xpw6PTf1bGfT3nR6nY+niprkjoZx8m0U1b
          wlL7dho3U2RHTshq6pNdH4+NRlSf4uC2a3OhJRUXcvdQBG1/Bo1QrjOT6pz6VQ1f
          h/chafWpjTAyw7zI66KH7f10zpXwDYg87Im1ro5emPrHWLscaHzm9BUZndi4VyFm
          FwX7Ddv7HGUsCfnuW7MvGPzF4BkVcGfWjhySCsYPz2Q2qJ9gToMItZX7dEv20e2f
          9P8LIzGCKYCBJM2PWQ1EeaOw3/+0aFfqEyxa2Ht26I6+SPTJa8vVxC13KAmB2qsM
          Ksly5oef7dE65pBODTgAuNxzf3+PxryB20u8RIXp9Ga7/nQPRgD0hvqNsVZ1LjXP
          uhm+KS1fXw/1RNZRZ8CvZlGfx32bkokR/Y0Mdq3WKT4/njbqUUe9gu3wkSCq/3H+
          RdJ3ECmue2qrz9ntPgFqBX4DRtF74JSKuEz/gZ7BGZo1VujMPJdjoDRCJrTHcAg6
          B5MImi/J5qoyWiSRBtjNTBfvMTrCgu/vedITb9wvPLTNV5tHHK020ZRY2fqVeKdc
          zY8HaHaHB0RUeGAUZMDLJ4WuuDIU57DVrYNTCWA2lPwhfH1hOe0CXLgDCa77pxZx
          UUpM/BEs+GbS8xW6UdyaBLBVMrut0/9cd5hiowY8+ftpO4r+JN+mLRmFOVyamEzx
          lzy9WX1LdaSNxwWXRsmQkCFsU01953tggJgxQVvNB0clt99D0p9EOJr4Kt5UPmzY
          4U0CYQ7tLUPPUaX1dTvnIL8Gn29tR7IfJHo/Bxa6sNx06iZ4+m22wkNtbdfe0RoE
          gPZb8g+4RUywhkvhszBbgDjRQTCwu59eYAWN8HH52dDnE3Vw+ql93RSFhK6wS/Dc
          WRFaHpNpnvapztvcsPwgDATNERjromjgKxk8QKcodx1/MWsa3/Qd51RFCXM7qYp1
          jCskzxul6U+rAxgpKzrU+m1dzqRGt1BjrQ==
          -----END CERTIFICATE-----

    clusters:  &bbm-clusters
      infra:
        service:
          domain: infra.corp.arista.io
        bsy:
          url: http://barney-bsy.barney:8181
        resources:
          requests:
            cpu: "1"
            memory: "2G"
          limits:
            cpu: "1"
            memory: "2G"
        config:
          cache-root: /data2/buscache
      engprod-dr:
        disable: true

  bbm-head:
    # Barney Bus Manager HEAD

    source:
      repoURL: https://github.com/barney-ci/bbm
      targetRevision: HEAD
      path: helm/bbm

    syncPolicy:
      automated: {}

    parameters: *bbm-parameters

    values:
      <<: *bbm-values
      service:
        weight: 0
      namespaceRbac:
        admin: k8s-infra-bus-admin@arista.com
        edit: k8s-infra-bus-edit@arista.com
        view: k8s-infra-bus-view@arista.com
      config:
        <<: *bbm-values-config
        controller: {}
        k8s:
          node-selector:
            node-role.kubernetes.io/bus-head: bus-head
          tolerations:
            type: bus:NoSchedule
            cgroups: v2:NoSchedule
        namespace: bus-head
        auth:
          client:
            id: ${BBM_AUTH_CLIENT_ID}
            secret: ${BBM_AUTH_CLIENT_SECRET}
      envSecrets:
        - bbm-dex-credentials
        - bbm-ldap-bind-account

    clusters: *bbm-clusters

  kafka-cluster:
    source:
      repoURL: https://github.com/aristanetworks/barney-ops
      targetRevision: HEAD
      path: helm-kafka
    syncPolicy:
      automated: {}
    clusters:
      infra:
        kafka:
          cluster:
            replicas: 5
            service:
              domain: infra.corp.arista.io
            resources:
              # Resource usage measured Jan 2025. Memory limit based on
              # container_memory_working_set_bytes.
              requests:
                cpu: "2"
                memory: 20Gi
              limits:
                memory: 40Gi

        zookeeper:
          replicas: 5
          resources:
            # Resource usage measured Jan 2025. Memory limit based on
            # container_memory_working_set_bytes.
            requests:
              cpu: "500m"
              memory: 2Gi
            limits:
              cpu: "1"
              memory: 5Gi
      engprod-dr:
        kafka:
          cluster:
            replicas: 3
            service:
              domain: engprod-dr.corp.arista.io
        zookeeper:
          replicas: 2

  rits:
    # Repo Integration Test Service - See AID9437
    source:
      repoURL: https://github.com/aristanetworks/rits
      targetRevision: 34ce54b8b2a67c6dc414691467a90772d1b3ab7e
      path: helm/rits

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}
      - name: image.archSuffix
        value: ""

    syncPolicy:
      automated:
        prune: true

    clusters:
      infra:
        service:
          domain: infra.corp.arista.io
      engprod-dr:
        disable: true

    values:
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
          ephemeral-storage: 2Gi
        requests:
          cpu: "100m"
          memory: 400Mi
          ephemeral-storage: 1Gi
      service:
        # also used for config.http.bind-address
        port: &ritsServicePort "8880"
        auth: true
      config:
        instance: "production"
        service-instances:
          - "production"
          - "testing"
          - "github"
        features:
          repomap-footprint:
            - 33702569 # wade.barneyRepomapFootprintEnable.0 -> eos-trunk
        storage:
          local-cache: true
          firestore:
            project: "sw-infra-barney"
            collection: "rits-topics"
            credentials:
              file: "/etc/secrets/firestore.json"
        kafka:
          p4-changes:
            topic: "gitarband-p4-changes"
            bootstrap: "ephemeral-cluster-kafka-bootstrap.strimzi-kafka-operator:9092"
            schema:
              registry: "http://cp-schema-registry.strimzi-kafka-operator:8081"
          sources:
            - topic: "barney-topics"
              type: "btm"
              bootstrap: "barney-kafka-cluster-kafka-bootstrap.barney:9092"
            - topic: "mut-kennel"
              type: "mut"
              bootstrap: "barney-kafka-cluster-kafka-bootstrap.barney:9092"
        checkers:
          - name: "horseland gerrit"
            url: "https://horseland-gerrit.infra.corp.arista.io"
            username: "srv-rits"
            password-file: "/etc/secrets/horseland-gerrit.txt"
          - name: "baremetal gerrit"
            url: "https://gerrit.corp.arista.io"
            username: "srv-rits"
            password-file: "/etc/secrets/gerrit.txt"
          - name: "github"
            url: https://github.com
            github:
              integration-id: "286826"
              private-key-file: /etc/secrets/github-app-private-key.pem
              v3-api-url: https://api.github.com
        oauth-app-credentials:
          client-id: "Ov23lido7FgN4oiEVxMY"
          client-secret-file: "/etc/secrets/githubOAuthAppSecret"
        github-ruleset-path: "/etc/rits/githubRuleset.yaml"
        metrics:
          # enable pyroscope, see also podAnnotations below.
          profiler:
            mutex-fraction: 1 # runtime.SetMutexProfileFraction(1)
            block-rate: 1     # runtime.SetBlockProfileRate(1)

      podAnnotations:
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/cpu.port: *ritsServicePort
        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/memory.port: *ritsServicePort
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/goroutine.port: *ritsServicePort
        profiles.grafana.com/mutex.scrape: "true"
        profiles.grafana.com/mutex.port: *ritsServicePort
        profiles.grafana.com/block.scrape: "true"
        profiles.grafana.com/block.port: *ritsServicePort

      githubRuleset: |+
        {
          "name": "rits-required",
          "target": "branch",
          "source_type": "Repository",
          "enforcement": "active",
          "conditions": {
            "ref_name": {
              "exclude": [],
              "include": [
                "~DEFAULT_BRANCH",
                "refs/heads/*"
              ]
            }
          },
          "rules": [
            {
              "type": "required_status_checks",
              "parameters": {
                "strict_required_status_checks_policy": false,
                "do_not_enforce_on_create": false,
                "required_status_checks": [
                  {
                    "context": "RITS",
                    "integration_id": 286826
                  }
                ]
              }
            }
          ],
          "bypass_actors": []
        }

  docker:
    source: &docker_source
      repoURL: https://github.com/barney-ci/docker-registry
      targetRevision: 0fdc82d834bd503f34b328e7c19042d3e04c05ec
      path: helm/docker-registry

    parameters: &docker_parameters
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}

    values: &docker_values
      image:
        repository: docker.corp.arista.io/barney-docker-registry
      repometadataURL: http://repometadata.barney:8080
      nameOverride: barney-docker
      service:
        weight: ""
      replicaCount: 1
      envSecrets:
        - barney-docker-dex-credentials
      strategy:
        type: RollingUpdate
      layerAmount: "40"

    syncPolicy:
      automated: {}

    clusters: &docker_clusters
      infra: &docker_clusters_infra
        apiURL: https://barney-api.infra.corp.arista.io
        domain: infra.corp.arista.io
        nodeSelector:
          node-role.kubernetes.io/barney-aux: barney-aux
        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule
      engprod-dr:
        disable: true
        image:
          repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/docker-registry
        apiURL: https://barney-api.engprod-dr.corp.arista.io
        domain: engprod-dr.corp.arista.io
        repometadataURL: http://repometadata.barney:8080

  docker-head:
    source:
      <<: *docker_source
      targetRevision: HEAD

    parameters: *docker_parameters

    values:
      <<: *docker_values
      nameOverride: barney-docker-head

    syncPolicy:
      automated: {}

    clusters:
      <<: *docker_clusters
      infra:
        <<: *docker_clusters_infra
        apiURL: http://barney-api-head.barney
      engprod-dr:
        disable: true

  ui:
    source:
      repoURL: https://github.com/barney-ci/qualia
      targetRevision: HEAD
      path: helm/barney-ui

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.  (Cargo culting this).
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}

    values:
      image:
        repository: barney-docker.infra.corp.arista.io/barney.ci/qualia

    syncPolicy:
      automated: {}

    clusters:
      infra:
        domain: infra.corp.arista.io
        replicaCount: 2
        nodeSelector:
          node-role.kubernetes.io/barney-aux: barney-aux
        service:
          annotations:
            getambassador.io/config: |
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney_ui_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney.infra.corp.arista.io
              prefix: /
              service: barney-ui.barney.svc.cluster.local.:3000
        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule
      engprod-dr:
        disable: true

  github-bridge: &github-bridge
    source:
      repoURL: https://github.com/barney-ci/github-bridge
      targetRevision: 48aa48760842744ae16db310009ef43d3fa1435e
      path: helm/github-bridge

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.  (Cargo culting this).
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}

    values:
      image:
        repository: barney-docker.infra.corp.arista.io/barney.ci/github-bridge
      envSecrets:
        - barney-github-bridge-dex-credentials
      appSecret: barney-ci-app

    syncPolicy:
      automated: {}

    clusters: &github-bridge-clusters
      infra: &github-bridge-clusters-infra
        resources:
          requests:
            cpu: 1
            memory: 1G
            ephemeral-storage: 500M
          limits:
            cpu: 1
            memory: 1G
        config: &github-bridge-clusters-infra-config
          queue-size: 1000
          num-workers: 100
          github:
            app:
              integration_id: 237227
          redirect-uri: "https://barney-github.infra.corp.arista.io/api/github/auth"
          api-server: https://barney-api.infra.corp.arista.io
          bsy-compat: https://bsy.infra.corp.arista.io
          redis:
            addr: redis-github-bridge.barney:6379
          repometadata: http://repometadata.barney:8080
          kafka: &github-bridge-clusters-infra-config-kafka
            brokers:
              - barney-kafka-cluster-kafka-bootstrap.barney:9092
            topic: github
        nodeSelector:
          node-role.kubernetes.io/barney-aux: barney-aux
        service:
          annotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney_github_bridge_root_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-github.infra.corp.arista.io
              prefix: /
              service: barney-github-bridge.barney.svc.cluster.local.:8080
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney_github_bridge_webhook_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-github.infra.corp.arista.io
              prefix: /github-webhook
              rewrite: /api/github/hook
              service: barney-github-bridge.barney.svc.cluster.local.:8080
        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule
      engprod-dr: &github-bridge-clusters-engprod-dr
        disable: true
        image:
          repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/github-bridge
        resources:
          requests:
            cpu: 1
            memory: 1G
            ephemeral-storage: 500M
          limits:
            cpu: 1
            memory: 1G
        config: &github-bridge-clusters-engprod-dr-config
          queue-size: 1000
          num-workers: 100
          github:
            app:
              integration_id: 237227
          redirect-uri: "https://barney-github.engprod-dr.corp.arista.io/api/github/auth"
          api-server: https://barney-api.engprod-dr.corp.arista.io
          bsy-compat: https://bsy.engprod-dr.corp.arista.io
          redis:
            addr: redis-github-bridge.barney:6379
          repometadata: http://repometadata.barney:8080
          kafka: &github-bridge-clusters-engprod-dr-config-kafka
            brokers:
              - barney-kafka-cluster-kafka-bootstrap.barney:9092
            topic: github
        service:
          annotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney_github_bridge_root_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-github.engprod-dr.corp.arista.io
              prefix: /
              service: barney-github-bridge.barney.svc.cluster.local.:8080
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney_github_bridge_webhook_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-github.engprod-dr.corp.arista.io
              prefix: /github-webhook
              rewrite: /api/github/hook
              service: barney-github-bridge.barney.svc.cluster.local.:8080

  github-bridge-shadow:
    <<: *github-bridge
    source:
      repoURL: https://github.com/barney-ci/github-bridge
      targetRevision: wade.bridgeHelthzTracing
      path: helm/github-bridge

    clusters:
      <<: *github-bridge-clusters
      infra:
        disable: true
        <<: *github-bridge-clusters-infra
        config:
          <<: *github-bridge-clusters-infra-config
          api-server: null
          bsy-compat: null
          redis: null
          repometadata: null
          kafka:
            <<: *github-bridge-clusters-infra-config-kafka
            topic: github-shadow
        service:
          annotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney_github_bridge_shadow_root_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-github.infra.corp.arista.io
              shadow: true
              prefix: /
              service: barney-github-bridge-shadow.barney.svc.cluster.local.:8080
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney_github_bridge_shadow_webhook_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-github.infra.corp.arista.io
              shadow: true
              prefix: /github-webhook
              rewrite: /api/github/hook
              service: barney-github-bridge-shadow.barney.svc.cluster.local.:8080
      engprod-dr:
        disable: true
        <<: *github-bridge-clusters-engprod-dr
        config:
          <<: *github-bridge-clusters-engprod-dr-config
          api-server: null
          bsy-compat: null
          redis: null
          repometadata: null
          kafka:
            <<: *github-bridge-clusters-engprod-dr-config-kafka
            topic: github-shadow
        service:
          annotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney_github_bridge_shadow_root_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-github.engprod-dr.corp.arista.io
              shadow: true
              prefix: /
              service: barney-github-bridge-shadow.barney.svc.cluster.local.:8080
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney_github_bridge_shadow_webhook_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: barney-github.engprod-dr.corp.arista.io
              shadow: true
              prefix: /github-webhook
              rewrite: /api/github/hook
              service: barney-github-bridge-shadow.barney.svc.cluster.local.:8080

  b5pinger:
    source:
      repoURL: https://github.com/barney-ci/pinger
      targetRevision: 4b7bc22f8b8b30449e9a48fc7e4c2b681dad87e9
      path: helm/b5pinger

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}
    # The deployment of pinger should target it's own fuzzytree version
    # this makes things less confusing as simply reverting targetRevision
    # will also affect the version of those images. Grep for FUZZYTREE_PARAMS
    - name: config.targets.fuzzytree-static.barneyref
      value: barney.ci/pinger%fuzzytree#${ARGOCD_APP_REVISION}
    - name: config.targets.fuzzytree-static-bsy-head.barneyref
      value: barney.ci/pinger%fuzzytree#${ARGOCD_APP_REVISION}

    syncPolicy:
      automated: {}

    values: &b5pinger_values
      image:
        repository: barney-docker.infra.corp.arista.io/barney.ci/pinger
      environment:
        - name: METRICS_COLLECTOR_HOST
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: METRICS_COLLECTOR_PORT
          value: "6626"
        - name: OTEL_EXPORTER_JAEGER_AGENT_HOST
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
      resources:
        limits:
          cpu: 2
          memory: 2Gi
        requests:
          cpu: 2
          memory: 2Gi
      config:
        bindaddr: 0.0.0.0:8080
        defaults:
          oauth-client: &pinger_client
            client-id: ${B5_PINGER_AUTH_CLIENT_ID}
            client-secret: ${B5_PINGER_AUTH_CLIENT_SECRET}
          barneyref: builtin%bootstrap
          timeout: 1h
          url: https://barney-api.infra.corp.arista.io
          interval: 5m
          concurrency: 3
          execlog: true
        stagger: 15s
        tracername: b5pinger

      envSecrets:
        - barney-b5-pinger-dex-credentials

    clusters:
      infra:
        config:
          influxaddr: ${METRICS_COLLECTOR_HOST}:${METRICS_COLLECTOR_PORT}
          targets:
            # Test the prod cluster. Note that this cluster runs with pretty
            # high scale so no individual node should be all that loaded.
            prod-static:
              interval: 1m
              concurrency: 10
            prod-rand:
              interval: 2m
              concurrency: 10
              randomize-hash-seed: true

            weighter-expr-test-static:
              interval: 5m
              concurrency: 10
              alt-clusters:
                JobPost-x86_64: worker-x86_64-weighter-expr-test

            # Test repomap generation from trailers/notes.
            repomap-prod-static:
              type: repomap
              barneyref: barney.ci/benchmarks/repomap
              repomap-digest: 65ad3e3585f5d8f615fd8e4f968b07daf06d16ad
              interval: 2m
            repomap-prod-rand:
              type: repomap
              barneyref: barney.ci/benchmarks/repomap
              repomap-digest: 65ad3e3585f5d8f615fd8e4f968b07daf06d16ad
              interval: 5m
              randomize-hash-seed: true
            repomap-head-static:
              type: repomap
              barneyref: barney.ci/benchmarks/repomap
              repomap-digest: 65ad3e3585f5d8f615fd8e4f968b07daf06d16ad
              url: https://barney-api-head.infra.corp.arista.io
              alt-clusters:
                Repomap: worker-x86_64-head
              interval: 2m
            repomap-head-rand:
              type: repomap
              barneyref: barney.ci/benchmarks/repomap
              repomap-digest: 65ad3e3585f5d8f615fd8e4f968b07daf06d16ad
              url: https://barney-api-head.infra.corp.arista.io
              alt-clusters:
                Repomap: worker-x86_64-head
              interval: 2m
              randomize-hash-seed: true

            # Test repomap generation from barneycontext file.
            repomap-prod-bctx-static:
              type: repomap
              barneyref: barney.ci/benchmarks/repomap#barneycontext
              repomap-digest: 048bc9597bc66dba22aeb3ba6b014de3dea34d8f
              interval: 2m
            repomap-prod-bctx-rand:
              type: repomap
              barneyref: barney.ci/benchmarks/repomap#barneycontext
              repomap-digest: 048bc9597bc66dba22aeb3ba6b014de3dea34d8f
              interval: 5m
              randomize-hash-seed: true
            repomap-head-bctx-static:
              type: repomap
              barneyref: barney.ci/benchmarks/repomap#barneycontext
              repomap-digest: 048bc9597bc66dba22aeb3ba6b014de3dea34d8f
              url: https://barney-api-head.infra.corp.arista.io
              alt-clusters:
                Repomap: worker-x86_64-head
              interval: 2m
            repomap-head-bctx-rand:
              type: repomap
              barneyref: barney.ci/benchmarks/repomap#barneycontext
              repomap-digest: 048bc9597bc66dba22aeb3ba6b014de3dea34d8f
              url: https://barney-api-head.infra.corp.arista.io
              alt-clusters:
                Repomap: worker-x86_64-head
              interval: 5m
              randomize-hash-seed: true

            fuzzytree-static: &ping_target_fuzzytree_static
              # barneyref is overridden in the "parameters" section at
              # the top of the service spec. Grep for FUZZYTREE_PARAMS
              # barneyref: barney.ci/pinger%fuzzytree
              interval: 5m
              randomize-hash-seed: false
              fnv-content-hash: "d2897821fdfaeafa"

            fuzzytree-static-bsy-head:
              # barneyref is overridden in the "parameters" section at
              # the top of the service spec. Grep for FUZZYTREE_PARAMS
              # barneyref: barney.ci/pinger%fuzzytree
              interval: 5m
              randomize-hash-seed: false
              fnv-content-hash: "d2897821fdfaeafa"
              alt-clusters:
                JobPost-x86_64: worker-x86_64-head
                Snapshot: worker-x86_64-head-random-all

            inode-exhaustion-test:
              interval: 2m
              timeout: 10m
              type: recipe
              alt-clusters:
                JobPost-x86_64: worker-x86_64-random-one
              # When the inode of a file exceeds 2^32 we see undefined behavior from
              # 32-bit binaries that aren't 64-bit inode aware. Generally this looks
              # as if the file doesn't exist. This test actually fails when the inode
              # for new files exceeds 80% of 2^32 (2^32 * 0.8 = 3435973836) so that when
              # the ping target starts failing we have time before the node starts
              # failing to run certain 32 bit binaries.
              recipe: |
                {
                  "description": "Test that new files have inode number under 2^32 * 0.8",
                  "steps": [
                    {
                      "mappings": {
                        "/": {"snapshot": "builtin%bootstrap"},
                        "/src/new-file": {
                          "snapshot": {
                            "description": "Create a new file (with a new inode)",
                            "steps": [
                              {
                                "mappings": {
                                  "/": {"snapshot": "builtin%bootstrap"},
                                  "/dest": {"output": "self", "mutable": true}
                                },
                                "commands": [
                                  [
                                    "sh",
                                    "-exc",
                                    "echo '%{PINGER_RANDOM_HASH_SEED}' >  /dest/inode-test"
                                  ]
                                ]
                              }
                            ]
                          }
                        }
                      },
                      "commands": [
                        [
                          "sh",
                          "-exc",
                          "test \"$(stat -c %i /src/new-file/inode-test)\" -lt \"3435973836\""
                        ]
                      ]
                    }
                  ]
                }

            # Testing the eext-bundle build across a variety of contexts
            # in x86
            eext-bundle-static-x86_64: &eext_bundle_base
              barneyref: code.arista.io/eos/eext-bundle%bundle-x86_64
              workspace-repos:
                - repo: code.arista.io/infra/barney/barnzilla-repos
                  priority: 100
              interval: 30m
              timeout: 3h
              concurrency: 2
            eext-bundle-static-aarch64:
              <<: *eext_bundle_base
              barneyref: code.arista.io/eos/eext-bundle%bundle-aarch64
              alt-clusters:
                JobPost-x86_64: worker-aarch64
                Snapshot: worker-aarch64-random-all

            eext-bundle-static-bsy-head-x86_64:
              <<: *eext_bundle_base
              alt-clusters:
                JobPost-x86_64: worker-x86_64-head
                Snapshot: worker-x86_64-head-random-all
            eext-bundle-static-bsy-head-aarch64:
              <<: *eext_bundle_base
              barneyref: code.arista.io/eos/eext-bundle%bundle-aarch64
              alt-clusters:
                JobPost-x86_64: worker-aarch64-head
                Snapshot: worker-aarch64-head-random-all

            # Test the api-head deployment while using the prod version of
            # everything else. Note that the `-static` target aught to be
            # a cache it so it's not likely to cause much load.
            api-head-static:
              url: https://barney-api-head.infra.corp.arista.io
              interval: 5m
              concurrency: 10

            # Test the bsy-head deployment while using the prod version of
            # everything else. Note that we pretty aggressively build the
            # ref with a random hash seed because we want to verify that the
            # head deployment can actually build novel snapshots. The head
            # deployment doesn't have much other traffic so this shouldn't
            # put undue burdon on the node.
            bsy-head-static:
              interval: 2m
              concurrency: 10
              alt-clusters:
                JobPost-x86_64: worker-x86_64-head
                Snapshot: worker-x86_64-head-random-all
            bsy-head-rand:
              interval: 2m
              concurrency: 10
              randomize-hash-seed: true
              alt-clusters:
                JobPost-x86_64: worker-x86_64-head
                Snapshot: worker-x86_64-head-random-all
            bsy-head-alpinetest-rand:
              barneyref: barney.ci/alpine%test#HEAD
              interval: 10m
              concurrency: 10
              randomize-hash-seed: true
              alt-clusters:
                JobPost-x86_64: worker-x86_64-head
                Snapshot: worker-x86_64-head-random-all

            # Test the arm worker nodes. Note that this cluster is smaller than
            # the x86 cluster and it does serve production traffic so we need
            # to be careful to not overwhelm it with robot-traffic.
            bsy-aarch64-static:
              interval: 2m
              concurrency: 10
              alt-clusters:
                # TODO: It would be nice if we could specify
                # --extra=arch=aarch64 instead of overriding
                # the x86_64 cluster but this will fill our
                # coverage gap in the short term.
                JobPost-x86_64: worker-aarch64
                Snapshot: worker-aarch64-random-all
            bsy-aarch64-rand:
              interval: 2m
              concurrency: 10
              randomize-hash-seed: true
              alt-clusters:
                JobPost-x86_64: worker-aarch64
                Snapshot: worker-aarch64-random-all
            # This target is identical to bsy-aarch64-static
            # except that we do not use the weighted aarch64
            # worker selector. We instead use pods at random. This
            # lets us compare the weighter expression with a baseline
            # and also ensures that all pods get at least some traffic.
            bsy-aarch64-unweighted-static:
              interval: 5m
              alt-clusters:
                # TODO: It would be nice if we could specify
                # --extra=arch=aarch64 instead of overriding
                # the x86_64 cluster but this will fill our
                # coverage gap in the short term.
                JobPost-x86_64: worker-aarch64-random-one
                Snapshot: worker-aarch64-random-all

            # Test the aarch64-head node. This guy doesn't get much production
            # traffic so we can get somewhat aggressive with load here.
            bsy-aarch64-head-static:
              interval: 2m
              concurrency: 10
              alt-clusters:
                JobPost-x86_64: worker-aarch64-head
                Snapshot: worker-aarch64-head-random-all
            bsy-aarch64-head-rand:
              interval: 2m
              concurrency: 10
              randomize-hash-seed: true
              alt-clusters:
                JobPost-x86_64: worker-aarch64-head
                Snapshot: worker-aarch64-head-random-all
            bsy-aarch64-head-alpinetest-rand:
              # NOTE: This needs to be set to HEAD after bugs in b5 portable client
              # get fixed
              barneyref: barney.ci/alpine%test#HEAD
              interval: 30m
              concurrency: 2
              randomize-hash-seed: true
              alt-clusters:
                JobPost-x86_64: worker-aarch64-head
                Snapshot: worker-aarch64-head-random-all
            bsy-aarch64-head-eext-bundle-rand:
              # NOTE: This needs to be set to HEAD after 1014831 gets fixed
              barneyref: code.arista.io/eos/eext-bundle%bundle-common#b80bca5275de953c4aa3fcb38e15a1142b8ae726
              workspace-repos:
                - repo: code.arista.io/eos/eext-bundle
                - repo: code.arista.io/infra/barney/barnzilla-repos
                  priority: 100
              interval: 1h
              timeout: 3h
              concurrency: 2
              randomize-hash-seed: true
              alt-clusters:
                JobPost-x86_64: worker-aarch64-head
                Snapshot: worker-aarch64-head-random-all

            # Test the job-compat-head deployment while using the prod
            # version of everything else.
            job-compat-head-static:
              interval: 5m
              concurrency: 10
              alt-clusters:
                JobStatus: job-status-head

            exp-failure-reason-match-static:
              url: https://barney-api.infra.corp.arista.io
              interval: 10m
              barneyref: barney.ci/pinger%expect-failure#HEAD
              expect-failure-log-re: |-
                (?s)THIS IS THE IMAGE THAT SHOULD FAIL
                .*\+ \[ build step exited with exit code 8 \]
            exp-failure-no-image-static:
              url: https://barney-api.infra.corp.arista.io
              interval: 10m
              # NOTE: This needs to be set to head after b5 portable client is fixed
              barneyref: barney.ci/pinger%noexist#d4e89f2fe611fe380dcc22fad92fd8c0baabf6cf
              expect-failure-log-re: 'bfg: no definition for image named "noexist"'

            exp-failure-reason-match-rand-bsy-head:
              url: https://barney-api.infra.corp.arista.io
              interval: 5m
              barneyref: barney.ci/pinger%expect-failure#HEAD
              randomize-hash-seed: true
              expect-failure-log-re: |-
                (?s)THIS IS THE IMAGE THAT SHOULD FAIL
                .*\'\] on ref: barney\.ci\/pinger%internal\/exp-fail-floor-commondep: exit status 8 \]
              alt-clusters:
                JobPost-x86_64: worker-x86_64-head
                Snapshot: worker-x86_64-head-random-all
            exp-failure-no-image-rand-bsy-head:
              url: https://barney-api.infra.corp.arista.io
              interval: 5m
              # NOTE: This needs to be set to head after fixes in b5 portable client
              barneyref: barney.ci/pinger%noexist#d4e89f2fe611fe380dcc22fad92fd8c0baabf6cf
              expect-failure-log-re: 'bfg: no definition for image named "noexist"'
              randomize-hash-seed: true
              alt-clusters:
                JobPost-x86_64: worker-x86_64-head
                Snapshot: worker-x86_64-head-random-all
            barnzilla-head-stress:
              interval: 15m
              timeout: 10m
              type: recipe
              recipe: |
                  {
                    "description": "Test the barnzilla-head service using barnzilla stress binary",
                    "repomap": {
                      "barney.ci/alpine": {"commit": "94fcebc578aebb21fdd40e7732df0d21c9ab3a73"},
                      "barney.ci/barneyfile": {"commit": "8b93658284ea9984354c4d5ffdaad6cb3531169b"},
                      "barney.ci/golang": {"commit": "dd612eba506a1c084a80920c879586c5ca196f7a"},
                      "github.com/golang/go": {"commit": "ace5bb40d027b718b67556afcd31bf54cff050ab"},
                      "code.arista.io/infra/barney/barnzilla": {"commit": "6680b829c0b3e02da71c9fe366ae5a1180c8ab75"}
                    },
                    "steps": [
                      {
                        "mappings": {"/": {"snapshot": "code.arista.io/infra/barney/barnzilla%stress-floor"}},
                        "entry": {
                          "env": {
                            "_IGNORED_HASH_SEED": "%{PINGER_RANDOM_HASH_SEED}",
                            "BZ_STRESS_BARNZILLA_URL": "https://barney-barnzilla-head.infra.corp.arista.io"
                          }
                        },
                        "commands": [["stress", "-test.v"]]
                      }
                    ]
                  }
            barnzilla-red-stress:
              interval: 33m
              timeout: 10m
              type: recipe
              recipe: |
                  {
                    "description": "Test the barnzilla-red service using barnzilla stress binary",
                    "repomap": {
                      "barney.ci/alpine": {"commit": "94fcebc578aebb21fdd40e7732df0d21c9ab3a73"},
                      "barney.ci/barneyfile": {"commit": "8b93658284ea9984354c4d5ffdaad6cb3531169b"},
                      "barney.ci/golang": {"commit": "dd612eba506a1c084a80920c879586c5ca196f7a"},
                      "github.com/golang/go": {"commit": "ace5bb40d027b718b67556afcd31bf54cff050ab"},
                      "code.arista.io/infra/barney/barnzilla": {"commit": "1f54820b8f6f8a2c83d217737c4d5f7c9a37179a"}
                    },
                    "steps": [
                      {
                        "mappings": {"/": {"snapshot": "code.arista.io/infra/barney/barnzilla%stress-floor"}},
                        "entry": {
                          "env": {
                            "_IGNORED_HASH_SEED": "%{PINGER_RANDOM_HASH_SEED}",
                            "BZ_STRESS_BARNZILLA_URL": "https://barney-barnzilla-red.infra.corp.arista.io"
                          }
                        },
                        "commands": [["stress", "-test.v"]]
                      }
                    ]
                  }
            barnzilla-green-stress:
              interval: 33m
              timeout: 10m
              type: recipe
              recipe: |
                  {
                    "description": "Test the barnzilla-green service using barnzilla stress binary",
                    "repomap": {
                      "barney.ci/alpine": {"commit": "94fcebc578aebb21fdd40e7732df0d21c9ab3a73"},
                      "barney.ci/barneyfile": {"commit": "8b93658284ea9984354c4d5ffdaad6cb3531169b"},
                      "barney.ci/golang": {"commit": "dd612eba506a1c084a80920c879586c5ca196f7a"},
                      "github.com/golang/go": {"commit": "ace5bb40d027b718b67556afcd31bf54cff050ab"},
                      "code.arista.io/infra/barney/barnzilla": {"commit": "1f54820b8f6f8a2c83d217737c4d5f7c9a37179a"}
                    },
                    "steps": [
                      {
                        "mappings": {"/": {"snapshot": "code.arista.io/infra/barney/barnzilla%stress-floor"}},
                        "entry": {
                          "env": {
                            "_IGNORED_HASH_SEED": "%{PINGER_RANDOM_HASH_SEED}",
                            "BZ_STRESS_BARNZILLA_URL": "https://barney-barnzilla-green.infra.corp.arista.io"
                          }
                        },
                        "commands": [["stress"]]
                      }
                    ]
                  }

            # temporary tests for https://discourse.arista.com/t/bsy-jobs-failing-intermittently-on-finalizer-using-artifactory/22879
            bsy-brcm-baseos:
              barneyref: code.arista.io/brcm/baseos%internal/dev-floor#a98b66beb5d678169d65f9908a13c6df498156c3
              interval: 10m
              concurrency: 2
              randomize-hash-seed: true
            b5-SSE-debug-recipe:
              interval: 60s
              timeout: 30m
              concurrency: 10
              type: recipe
              recipe: |
                  {
                    "description": "test recipe; run a simple command on an alpine floor",
                    "repomap": {
                      "barney.ci/alpine": {
                        "commit": "fb1ae6f324425a5f9260d60d7cc113146bb97913"
                      }
                    },
                    "steps": [
                      {
                        "mappings": {
                          "/": {
                            "snapshot": "barney.ci/alpine%internal/bootstrap"
                          }
                        },
                        "commands": [
                          [
                            "sleep", "25m"
                          ]
                        ]
                      }
                    ]
                  }

            # Experimental targets
            check-i386:
              type: job
              interval: 10m
              timeout: 20m
              barneyref: code.arista.io/util/linux/atfork%export
              workspace-repos:
                - repo: code.arista.io/util/linux/atfork#HEAD
                - repo: code.arista.io/infra/barney/barnzilla-repos#refs/heads/eos-trunk-i386_el9
                  priority: 100
              files:
                - usr/include/AtFork/AtFork.h
                - usr/lib/libAtFork.so
            check-x86_64:
              type: job
              interval: 10m
              timeout: 20m
              barneyref: code.arista.io/util/linux/atfork%export
              workspace-repos:
                - repo: code.arista.io/util/linux/atfork#HEAD
                - repo: code.arista.io/infra/barney/barnzilla-repos#HEAD
                  priority: 100
              files:
                - usr/include/AtFork/AtFork.h
                - usr/lib64/libAtFork.so
            check-i386-head:
              type: job
              interval: 10m
              timeout: 20m
              url: https://barney-api-head.infra.corp.arista.io
              barneyref: code.arista.io/util/linux/atfork%export
              workspace-repos:
                - repo: code.arista.io/util/linux/atfork#HEAD
                - repo: code.arista.io/infra/barney/barnzilla-repos#refs/heads/eos-trunk-i386_el9
                  priority: 100
              files:
                - usr/include/AtFork/AtFork.h
                - usr/lib/libAtFork.so
              alt-clusters:
                JobPost-x86_64: worker-x86_64-head
                Snapshot: worker-x86_64-head-random-all
            check-x86_64-head:
              type: job
              interval: 10m
              timeout: 20m
              url: https://barney-api-head.infra.corp.arista.io
              barneyref: code.arista.io/util/linux/atfork%export
              workspace-repos:
                - repo: code.arista.io/util/linux/atfork#HEAD
                - repo: code.arista.io/infra/barney/barnzilla-repos#HEAD
                  priority: 100
              files:
                - usr/include/AtFork/AtFork.h
                - usr/lib64/libAtFork.so
              alt-clusters:
                JobPost-x86_64: worker-x86_64-head
                Snapshot: worker-x86_64-head-random-all

            bld-mgr-head-build:
              type: build
              url: https://barney-api.infra.corp.arista.io
              interval: 10m
              timeout: 30m
              concurrency: 30
              barneyref: barney.ci/barney%static#HEAD
              randomize-hash-seed: false
              alt-clusters:
                PutBuild: build-head
                GetBuild: build-head
            bld-mgr-build: &pinger_bld_mgr_build
              type: build
              url: https://barney-api.infra.corp.arista.io
              interval: 10m
              timeout: 30m
              concurrency: 30
              barneyref: barney.ci/barney%static#256696cbb752713ed235d2d139f8d2a3bafcf87b
              randomize-hash-seed: false
              alt-clusters:
                PutBuild: build
                GetBuild: build

        domain: infra.corp.arista.io
        service:
          weight: ""
          annotations:
            getambassador.io/config: |
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney_b5pinger_direct_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: b5pinger.infra.corp.arista.io
              prefix: /
              service: barney-b5pinger.barney.svc.cluster.local.:8080
              timeout_ms: 60000
              idle_timeout_ms: 60000
        labels:
          environment: production
        nodeSelector:
          node-role.kubernetes.io/barney-aux: barney-aux
        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule
      engprod-dr:
        disable: true

  b5pinger-head:
    source:
      repoURL: https://github.com/barney-ci/pinger
      targetRevision: HEAD
      path: helm/b5pinger

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}
    # The deployment of pinger should target it's own fuzzytree version
    # this makes things less confusing as simply reverting targetRevision
    # will also affect the version of those images. Grep for FUZZYTREE_PARAMS
    - name: config.targets.fuzzytree-static.barneyref
      value: barney.ci/pinger%fuzzytree#${ARGOCD_APP_REVISION}

    syncPolicy:
      automated: {}

    values:
      <<: *b5pinger_values
      secrets:
        gitCredentials: bsy-git-credentials

    clusters:
      infra:
        config:
          defaults:
            oauth-client:
              <<: *pinger_client
            interval: 5m
            concurrency: 2
            timeout: 10m
            url: https://barney-api.infra.corp.arista.io
          target-prefix: "b5pinger-head-"
          influxaddr: ${METRICS_COLLECTOR_HOST}:${METRICS_COLLECTOR_PORT}
          targets:
            recipe:
              type: recipe
              recipe: |
                  {
                    "description": "test recipe; run a simple command on an alpine floor",
                    "repomap": {
                      "barney.ci/alpine": {
                        "commit": "fb1ae6f324425a5f9260d60d7cc113146bb97913"
                      }
                    },
                    "steps": [
                      {
                        "mappings": {
                          "/": {
                            "snapshot": "barney.ci/alpine%internal/bootstrap"
                          }
                        },
                        "commands": [
                          [
                            "cat", "--help"
                          ]
                        ]
                      }
                    ]
                  }
            static:
            repomap-static:
              type: repomap
              barneyref: barney.ci/benchmarks/repomap#97f97ae8e814d99c60183ace489606f0a639ebc3
              repomap-digest: 65ad3e3585f5d8f615fd8e4f968b07daf06d16ad

            bld-mgr-build:
              <<: *pinger_bld_mgr_build

            fuzzytree-static:
              <<: *ping_target_fuzzytree_static

        domain: infra.corp.arista.io
        service:
          weight: ""
        nodeSelector:
          node-role.kubernetes.io/barney-aux: barney-aux
        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule
      engprod-dr:
        disable: true

  kafka-ui:
    source:
      repoURL: https://github.com/kafbat/helm-charts.git
      # https://github.com/kafbat/helm-charts/releases/tag/charts%2Fkafka-ui-1.4.10
      #  version: 1.4.10
      #  appVersion: 1.1.0
      targetRevision: cb3a0b14097cf507827dff59e00fbd14b6538940
      path: charts/kafka-ui

    parameters:
      - name: image.tag
        value: 4cf17a0b2b17bdebd533caffd06978180b29c0ab@sha256:050543568675d5f1c34f6bb62c47720f700cf7baa1a4cf7bb7c38babed2fed7f

    syncPolicy:
      automated: {}

    values:
      envs:
        config:
          KAFKA_CLUSTERS_0_NAME: "barney"
          KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: "barney-kafka-cluster-kafka-bootstrap.barney:9092"
          KAFKA_CLUSTERS_0_READONLY: "true"

      serviceAccount:
        create: false

    clusters:
      infra:
        service:
          annotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney_kafka_ui_http_mapping
              ambassador_id:
              - internal_proxy
              service: barney-kafka-ui.barney.svc.cluster.local.
              host: "barney-kafka-ui.infra.corp.arista.io"
              timeout_ms: 600000
              idle_timeout_ms: 600000
              prefix: /
      engprod-dr:
        disable: true

  ganimede:
    source:
      repoURL: https://github.com/aristanetworks/ganimede
      targetRevision: HEAD
      path: helm/ganimede

    parameters:
    # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
    # is unable to substitute in a `values` string.  (Cargo culting this).
    - name: image.tag
      value: ${ARGOCD_APP_REVISION}

    values:
      resources:
        limits:
          cpu: 4
          memory: 10Gi
        requests:
          cpu: 2
          memory: 1Gi

      nodeSelector:
        node-role.kubernetes.io/barney: barney

      tolerations:
        - key: type
          operator: Equal
          value: barney
          effect: NoSchedule

      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - topologyKey: "kubernetes.io/hostname"
              labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values:
                      - barney-ganimede

      secrets:
        gitCredentials: bsy-git-credentials
        environment: bsy-secrets

    syncPolicy:
      automated: {}

    clusters:
      infra:
        # NOTE: do not scale this unless you found a solution
        # to ambassador being utterly broken in the face of
        # the ring_hash load balancer. Currently, all websocket
        # connections get routed to different hosts, and supposedly
        # using the endpoint resolver + the ring_hash load balancer
        # policy + source_ip == true is supposed to make this work
        # but the reality is that as soon as I apply the config
        # mentioned in the documentation, the entire service breaks.
        replicaCount: 1

        bsy:
          cacheName: 'barney-bsy'
          configName: 'barney-bsy-b5-conf'
          nodeCachePath: '/data2/b5cache'

        service:
          annotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  ganimede_http_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: ganimede.infra.corp.arista.io
              prefix: /
              service: barney-ganimede.barney.svc.cluster.local.:3000
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  ganimede_http_port_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: ganimede.infra.corp.arista.io:443
              prefix: /
              service: barney-ganimede.barney.svc.cluster.local.:3000
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  ganimede_wss_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: ganimede.infra.corp.arista.io
              use_websocket: true
              prefix: /socket.io/
              rewrite: /socket.io/
              service: barney-ganimede.barney.svc.cluster.local.:3000
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  ganimede_wss_port_mapping
              ambassador_id: [ internal_proxy ]
              bypass_auth: true
              host: ganimede.infra.corp.arista.io:443
              use_websocket: true
              prefix: /socket.io/
              rewrite: /socket.io/
              service: barney-ganimede.barney.svc.cluster.local.:3000

  barnzilla-redis:
    source:
      repoURL: https://github.com/aristanetworks/barney-ops
      targetRevision: HEAD
      path: helm-redis

    syncPolicy:
      automated: {}

    clusters:
      infra:
        name: "barnzilla-redis"
        replicas: 3
        # TODO: Remove this IP once we have transitioned.
        bootstrap: "192.168.175.233"
      engprod-dr:
        name: "barnzilla-redis"
        replicas: 2

  barnzilla-head:
    # barnzilla-head is synced automatically to HEAD
    <<: *barnzilla_config

    values:
      image:
        <<: *barnzilla_values_image
      config:
        <<: *barnzilla_values_config
        barnzilla:
          url: http://barney-barnzilla-head.barney.svc.cluster.local.:8002
      service:
        addAnnotations:
          getambassador.io/config: |
            ---
            apiVersion: getambassador.io/v2
            kind:  Mapping
            name:  barney-barnzilla-head_direct_http_mapping
            ambassador_id: [internal_proxy]
            bypass_auth: true
            host: barney-barnzilla-head.infra.corp.arista.io
            timeout_ms: 600000
            idle_timeout_ms: 600000
            prefix: /
            method: GET
            service: barney-barnzilla-head.barney.svc.cluster.local.:8002

    source:
      <<: *barnzilla_source
      targetRevision: HEAD
    clusters:
      <<: *barnzilla_clusters
      infra:
        <<: *barnzilla_infra
#       In case BARNZILLA_P4_SERVER is corrupt, we can override it here
#       environment:
#         - name: BARNZILLA_P4_SERVER
#           value: "p4-edge217.sjc.aristanetworks.com:1666"
        service:
          <<: *barnzilla_service
          weight: 0
          grpc: true
        replicaCount: 2
        labels:
          environment: experimental
      engprod-dr:
        disable: true

  # Please check NEXT_BARNZILLA_UPGRADE
  barnzilla-red:
    # This is the old version (weight decreasing)
    # Application that is part of the red/green deployment process.
    # Usually one application is stable (eg: red) while the other
    # one (eg: green) is a test version. targetRevision has to be
    # set manually.
    <<: *barnzilla_config

    source:
      <<: *barnzilla_source
      targetRevision: 93907118440a4f9addf1606bbf1be7a5f1e9b6b6
    values:
      <<: *barnzilla_values
    clusters:
      <<: *barnzilla_clusters
      infra:
        <<: *barnzilla_infra
        service:
          <<: *barnzilla_service
          # This deployment is __shrinking__ barnzilla_red_green
          # BARNZILLA_GREEN_RED
          grpc: true
          addAnnotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-barnzilla-red_direct_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barney-barnzilla-red.infra.corp.arista.io
              timeout_ms: 600000
              idle_timeout_ms: 600000
              prefix: /
              method: GET
              service: barney-barnzilla-red.barney.svc.cluster.local.:8002
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-barnzilla-red_global_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barnzilla.infra.corp.arista.io
              timeout_ms: 600000
              idle_timeout_ms: 600000
              prefix: /
              method: GET
              weight: 95
              service: barney-barnzilla-red.barney.svc.cluster.local.:8002
        # replicaCount has to be at least 1 so pinger barnzilla-red tests won't fail.
        replicaCount: 15
        labels:
          environment: production
      engprod-dr:
        <<: *barnzilla_engprod_dr
        service:
          <<: *barnzilla_engprod_dr_service
          grpc: true
          addAnnotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-barnzilla-red_global_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barnzilla.engprod-dr.corp.arista.io
              timeout_ms: 600000
              idle_timeout_ms: 600000
              prefix: /
              weight: 100
              service: barney-barnzilla-red.barney.svc.cluster.local.:8002
        replicaCount: 1
        labels:
          environment: production

  # NEXT_BARNZILLA_UPGRADE (assuming PR using /mnt/p4trust/p4trust merged)
  #
  # BARNZILLA_P4_SERVER will be provided by p4trust team. This way we don't
  # have to set it manually and in case of emergencies BARNZILLA_P4_SERVER
  # can be set by p4trust team. We have to remove this env however from
  # barney-ops before this happens.
  #
  # Notes for anyone about to upgrade barnzilla
  # [ x ] add BARNZILLA_P4_SERVER env to barnzilla-green.
  # [   ] deploy new barnzilla-green.
  # [   ] remove BARNZILLA_P4_SERVER env from barnzilla_values. barnzilla-green
  #       has its own override.
  # [   ] deploy new barnzilla-red. barnzilla-red is now using /mnt/p4trust/p4trust
  # [   ] remove BARNZILLA_P4_SERVER env from barnzilla-green
  # [   ] deploy new barnzilla-green. barnzilla-green is now using /mnt/p4trust
  barnzilla-green:
    # This is the new version (weight increasing)
    # Application that is part of the red/green deployment process.
    # Usually one application is stable (eg: red) while the other
    # one (eg: green) is a test version. targetRevision has to be
    # set manually.
    <<: *barnzilla_config
    source:
      <<: *barnzilla_source
      targetRevision: NOT_HEAD
    values:
      image:
        <<: *barnzilla_values_image
      config:
        <<: *barnzilla_values_config
        barnzilla:
          url: http://barney-barnzilla-green.barney.svc.cluster.local.:8002
    clusters:
      <<: *barnzilla_clusters
      infra:
        <<: *barnzilla_infra
        #       In case BARNZILLA_P4_SERVER is corrupt, we can override it here
        #       environment:
        #          - name: BARNZILLA_P4_SERVER
        #            value: "p4-edge217.sjc.aristanetworks.com:1666"
        service:
          <<: *barnzilla_service
          # This deployment is __growing__ barnzilla_red_green
          # BARNZILLA_GREEN_RED
          grpc: true
          addAnnotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-barnzilla-green_direct_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barney-barnzilla-green.infra.corp.arista.io
              timeout_ms: 600000
              idle_timeout_ms: 600000
              prefix: /
              method: GET
              service: barney-barnzilla-green.barney.svc.cluster.local.:8002
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-barnzilla-green_global_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barnzilla.infra.corp.arista.io
              timeout_ms: 600000
              idle_timeout_ms: 600000
              prefix: /
              method: GET
              weight: 5
              service: barney-barnzilla-green.barney.svc.cluster.local.:8002
        # replicaCount has to be at least 1 so pinger barnzilla-green tests won't fail.
        replicaCount: 2
        labels:
          environment: production
      engprod-dr:
        <<: *barnzilla_engprod_dr
        disable: true
        service:
          <<: *barnzilla_engprod_dr_service
          grpc: true
          addAnnotations:
            getambassador.io/config: |
              ---
              apiVersion: getambassador.io/v2
              kind:  Mapping
              name:  barney-barnzilla-green_global_http_mapping
              ambassador_id: [internal_proxy]
              bypass_auth: true
              host: barnzilla.engprod-dr.corp.arista.io
              timeout_ms: 600000
              idle_timeout_ms: 600000
              prefix: /
              weight: 0
              service: barney-barnzilla-green.barney.svc.cluster.local.:8002
        replicaCount: 1
        labels:
          environment: production

  earlyoom-head: &earlyoom
    # Earlyoom is a daemonset that intends to protect nodes from OOM
    # condition. It runs a userspace OOM killer that kills large
    # memory-heavy processes before the system runs out of memory.
    # earlyoom-head runs on the barney-test node while earlyoom runs
    # on all the other nodes.
    source:
      repoURL: https://github.com/aristanetworks/barney-ops
      targetRevision: HEAD
      path: helm-oomkiller
    syncPolicy:
      automated: {}

    values:
      earlyoom:
        # Regex intended to match any command where the argv0
        # contains `b5` --- both with and without an explicit
        # path. We match `b5-worker daemon`, and `/usr/bin/b5`
        # but not `cat /etc/b5.conf`.
        additionalArguments: "--avoid '^([^ ]*/)?(b5|setup|git|promtail|fw$)'"
        memory:
          sigtermPercent: 10
          sigkillPercent: 5
      tolerations:
        - key: type
          operator: Equal
          value: barney
          effect: NoSchedule

    clusters:
      infra:
        nodeSelector:
          node-role.kubernetes.io/barney-test: barney-test
      engprod-dr:
        disable: true

  earlyoom:
    <<: *earlyoom
    clusters:
      infra:
        nodeSelector:
          node-role.kubernetes.io/barney: barney
      engprod-dr:
        disable: false

  postgres:
    source:
      repoURL: https://github.com/aristanetworks/barney-ops
      targetRevision: HEAD
      path: helm-postgres
    syncPolicy:
      automated: {}
    clusters:
      infra:
        postgres:
          - name: barney-postgres-cluster
            namespace: barney
            labels: {}
            replicas: 2
            maxConnections: "90"
            storage:
              size: 100Gi
              storageClass: portworx-fada
            ressources:
              enabled: true
              requests:
                cpu: "1"
                memory: "2Gi"
              limits:
                cpu: "2"
                memory: "4Gi"
            preparedDatabases:
              jobstats:
                defaultUsers: true
            masterServiceAnnotations:
              getambassador.io/config: |
                ---
                apiVersion: getambassador.io/v2
                kind: TCPMapping
                port: 5432
                name: barney-postgres-cluster
                ambassador_id:
                - internal_proxy
                service: barney-postgres-cluster.barney.svc.cluster.local.:5432
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                    - key: node-role.kubernetes.io/worker
                      operator: In
                      values:
                        - worker
          - name: barney-postgres-cluster-head
            namespace: barney
            labels: {}
            replicas: 2
            maxConnections: "90"
            storage:
              size: 100Gi
              storageClass: portworx-fada
            ressources:
              enabled: true
              requests:
                cpu: "1"
                memory: "2Gi"
              limits:
                cpu: "2"
                memory: "4Gi"
            preparedDatabases:
              jobstatshead:
                defaultUsers: true
            masterServiceAnnotations:
              getambassador.io/config: |
                ---
                apiVersion: getambassador.io/v2
                kind: TCPMapping
                port: 5433
                name: barney-postgres-cluster-head
                ambassador_id:
                - internal_proxy
                service: barney-postgres-cluster-head.barney.svc.cluster.local.:5432
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                  - matchExpressions:
                    - key: node-role.kubernetes.io/barney-cache-service
                      operator: In
                      values:
                        - barney-cache-service
            tolerations:
              - key: type
                operator: Equal
                value: barney
                effect: NoSchedule
      engprod-dr:
        disable: true

  # External secrets that belong in the barney namespace
  external-secrets: &external_secrets
    source:
      repoURL: https://github.com/aristanetworks/barney-ops
      targetRevision: HEAD
      path: helm-external-secret

    syncPolicy:
      automated:
        prune: true

    clusters:
      infra:
        secrets:
          bsy-compat-dex-credentials:
            data:
            - secretKey: secret
              remoteRef:
                key: barney-bsy-compat--dex-client-secret--infra
            secret:
              BSY_COMPAT_AUTH_CLIENT_ID: 'ezq7flyh7tmxnv4c5dhsdwkb6'
              BSY_COMPAT_AUTH_CLIENT_SECRET: '{{ .secret | toString }}'
          barney-docker-dex-credentials:
            data:
            - secretKey: secret
              remoteRef:
                key: barney-docker--dex-client-secret--infra
            secret:
              DOCKER_REGISTRY_AUTH_CLIENT_ID: 'p3laz5lo56oyu5edgbml7ixa6'
              DOCKER_REGISTRY_AUTH_CLIENT_SECRET: '{{ .secret | toString }}'
          barney-github-bridge-dex-credentials:
            data:
            - secretKey: secret
              remoteRef:
                key: barney-github-bridge--dex-client-secret--infra
            secret:
              GITHUB_BRIDGE_AUTH_CLIENT_ID: 'piio67xak35wmp2l4ox54syp7'
              GITHUB_BRIDGE_AUTH_CLIENT_SECRET: '{{ .secret | toString }}'
          barney-b5-pinger-dex-credentials:
            data:
            - secretKey: secret
              remoteRef:
                key: barney-b5-pinger--dex-client-secret--infra
            secret:
              B5_PINGER_AUTH_CLIENT_ID: 'n7fygxfopjbm6snclikbkcq4n'
              B5_PINGER_AUTH_CLIENT_SECRET: '{{ .secret | toString }}'
          bbm-ldap-bind-account:
            data:
            - secretKey: password
              remoteRef:
                key: bbm--srv-bus-manager-password--infra
            secret:
              BBM_LDAP_BIND_ACCOUNT_USERNAME: 'CN=srv-bus-manager,OU=SrvOL,OU=Contractors,OU=Arista,DC=aristanetworks,DC=com'
              BBM_LDAP_BIND_ACCOUNT_PASSWORD: '{{ .password | toString }}'
          bbm-dex-credentials:
            data:
            - secretKey: secret
              remoteRef:
                key: bbm--dex-client-secret--infra
            secret:
              BBM_AUTH_CLIENT_ID: *bbm-auth-client-id
              BBM_AUTH_CLIENT_SECRET: '{{ .secret | toString }}'
          barnzilla-gitarband-key:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--barnzilla-gitarband-key--infra
            secret:
              barnzilla-gitarband-key: '{{ .secret | toString }}'
          barnzilla-gitarband-gerrit-key:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--barnzilla-gitarband-gerrit-key--infra
            secret:
              barnzilla-gitarband-gerrit-key: '{{ .secret | toString }}'
          barnzilla-arastra-key:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--barnzilla-arastra-key--infra
            secret:
              barnzilla-arastra-key: '{{ .secret | toString }}'
          barnzilla-p4-password:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--barnzilla-p4-password--infra
            secret:
              barnzilla-p4-password: '{{ .secret | toString }}' # This is kept for legacy reasons atm
              token: '{{ .secret | toString }}' # This is consumed by p4ticket-cron service
          btm-secrets:
            data:
            - secretKey: username
              remoteRef:
                key: barney--btm-github-username--infra
            - secretKey: password
              remoteRef:
                key: barney--btm-github-password--infra
            - secretKey: firestore
              remoteRef:
                key: barney--btm-firestore-credentials--infra
            secret:
              btm-github-password: '{{ .password | toString }}'
              btm-github-username: '{{ .username | toString }}'
              firestore.json: '{{ .firestore | toString }}'
          btm-head-secrets:
            data:
            - secretKey: firestore
              remoteRef:
                key: barney--btm-head-firestore-credentials--infra
            secret:
              firestore.json: '{{ .firestore | toString }}'
          bsy-secrets:
            data:
            - secretKey: jenkins-secret
              remoteRef:
                key: barney--jenkins-api-token--infra
            - secretKey: bbm-test-auth-client-secret
              remoteRef:
                key: bbm-test--dex-client-secret--infra
            secret:
              B5_HOST_SETTINGS_SECRETS_JENKINS: '{{ index . "jenkins-secret" | toString }}'
              B5_HOST_SETTINGS_SECRETS_BBM_AUTH_CLIENT_ID: *bbm-test-auth-client-id
              B5_HOST_SETTINGS_SECRETS_BBM_AUTH_CLIENT_SECRET: '{{ index . "bbm-test-auth-client-secret" | toString }}'
          bsy-git-credentials:
            data:
            - secretKey: ghToken
              remoteRef:
                key: barney--arista_srv_bessy_github_token--infra
            - secretKey: gerritToken
              remoteRef:
                key: barney--srv_bessy_gerrit_token--infra
            - secretKey: horselandGerritToken
              remoteRef:
                key: barney--srv_bessy_horseland_gerrit_token--infra
            secret:
              git-credentials: |
                https://arista-srv-bessy:{{ .ghToken | toString }}@github.com
                https://srv-bessy:{{ .gerritToken | toString }}@gerrit.corp.arista.io
                https://srv-bessy:{{ .horselandGerritToken | toString }}@horseland-gerrit.infra.corp.arista.io
          bsy-compat-secrets:
            data:
            - secretKey: gerritToken
              remoteRef:
                key: barney--srv_bessy_gerrit_token--infra
            - secretKey: horselandGerritToken
              remoteRef:
                key: barney--srv_bessy_horseland_gerrit_token--infra
            secret:
              GERRIT_USER: 'srv-bessy'
              GERRIT_PASSWORD_gerrit.corp.arista.io: "{{ .gerritToken | toString }}"
              GERRIT_PASSWORD_horseland-gerrit.infra.corp.arista.io: '{{ .horselandGerritToken | toString }}'
          bbm-creds:
            data:
            - secretKey: username
              remoteRef:
                key: barney--bbm-github-username--infra
            - secretKey: password
              remoteRef:
                key: barney--bbm-github-password--infra
            secret:
              GITHUB_USERNAME: '{{ .username | toString }}'
              GITHUB_PASSWORD: '{{ .password | toString }}'
          repometadata-firestore-credentials:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--repometadata-firestore-credentials--infra
            secret:
              repometadata-firestore-credentials: '{{ .secret | toString }}'
          repometadata-gerrit-key:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--repometadata-gerrit-key--infra
            secret:
              repometadata-gerrit-key: '{{ .secret | toString }}'
          repometadata-gerrit-username:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--repometadata-gerrit-username--infra
            secret:
              repometadata-gerrit-username: '{{ .secret | toString }}'
          repometadata-gerrit-api-password:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--repometadata-gerrit-api-password--infra
            secret:
              repometadata-gerrit-api-password: '{{ .secret | toString }}'
          repometadata-github-password:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--repometadata-github-password--infra
            secret:
              repometadata-github-password: '{{ .secret | toString }}'
          repometadata-github-username:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--repometadata-github-username--infra
            secret:
              repometadata-github-username: '{{ .secret | toString }}'
          rits-secrets:
            data:
            - secretKey: firestore
              remoteRef:
                key: barney--rits-firestore-credentials--infra
            - secretKey: gerrit
              remoteRef:
                key: barney--rits-gerrit-password--infra
            - secretKey: githubApp
              remoteRef:
                key: barney--rits-github-app-private-key--infra
            - secretKey: horselandGerrit
              remoteRef:
                key: barney--rits-horseland-gerrit-password--infra
            - secretKey: servicekey
              remoteRef:
                key: barney--rits-service-key--infra
            - secretKey: githubOAuthAppSecret
              remoteRef:
                key: barney--rits-github-oauth-app-secret--infra
            secret:
              firestore.json: '{{ .firestore | toString }}'
              gerrit.txt: '{{ .gerrit | toString }}'
              github-app-private-key.pem: '{{ .githubApp | toString }}'
              horseland-gerrit.txt: '{{ .horselandGerrit | toString }}'
              serviceKey: '{{ .servicekey | toString }}'
          bar-observer-environment:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--bar-observer-password--infra
            secret:
              BAR_OBSERVER_PASSWORD: '{{ .secret | toString }}'
          # TODO: Remove all multibar secrets from barney namespace
          # once multibar has been moved to barney-bar namespace
          multibar-alpha-secrets:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--multibar-alpha-password--infra
            - secretKey: username
              remoteRef:
                key: barney--multibar-alpha-username--infra
            secret:
              gitauth.yaml: |
                username:
                  value: {{ .username | toString | b64enc }}
                  encoding: base64
                password:
                  value: {{ .secret | toString | b64enc }}
                  encoding: base64
                ssh-key:
                  value:
                  encoding: base64
                ignore-host-keys: true
          multibar-barnzilla-repos-secrets:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--multibar-barnzilla-repos-password--infra
            - secretKey: username
              remoteRef:
                key: barney--multibar-barnzilla-repos-username--infra
            secret:
              gitauth.yaml: |
                username:
                  value: {{ .username | toString | b64enc }}
                  encoding: base64
                password:
                  value: {{ .secret | toString | b64enc }}
                  encoding: base64
                ssh-key:
                  value:
                  encoding: base64
                ignore-host-keys: true
          multibar-beta-secrets:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--multibar-beta-password--infra
            - secretKey: username
              remoteRef:
                key: barney--multibar-beta-username--infra
            secret:
              gitauth.yaml: |
                username:
                  value: {{ .username | toString | b64enc }}
                  encoding: base64
                password:
                  value: {{ .secret | toString | b64enc }}
                  encoding: base64
                ssh-key:
                  value:
                  encoding: base64
                ignore-host-keys: true
          multibar-firestore-secrets:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--multibar-firestore-secrets--infra
            secret:
              firestore.json: '{{ .secret | toString }}'
          multibar-production-github-secrets:
            data:
            - secretKey: sshkey
              remoteRef:
                key: barney--multibar-production-github-ssh_key--infra
            - secretKey: username
              remoteRef:
                key: barney--multibar-production-github-username--infra
            secret:
              gitauth.yaml: |
                username:
                  value: {{ .username | toString | b64enc }}
                  encoding: base64
                password:
                  value:
                  encoding: base64
                ssh-key:
                  value: {{ .sshkey | toString | b64enc }}
                  encoding: base64
                ignore-host-keys: true
          multibar-production-secrets:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--multibar-production-password--infra
            - secretKey: username
              remoteRef:
                key: barney--multibar-production-username--infra
            secret:
              gitauth.yaml: |
                username:
                  value: {{ .username | toString | b64enc }}
                  encoding: base64
                password:
                  value: {{ .secret | toString | b64enc }}
                  encoding: base64
                ssh-key:
                  value:
                  encoding: base64
                ignore-host-keys: true
          grl-github-secrets:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--grl-github-password--infra
            - secretKey: username
              remoteRef:
                key: barney--grl-github-username--infra
            secret:
              gitauth.yaml: |
                username:
                  value: {{ .username | toString | b64enc }}
                  encoding: base64
                password:
                  value: {{ .secret | toString | b64enc }}
                  encoding: base64
                ssh-key:
                  value:
                  encoding: base64
                ignore-host-keys: true
      engprod-dr:
        disable: true

  # External secrets that belong in the barney-bar namespace
  external-secrets-barney-bar:
    <<: *external_secrets

    namespace: barney-bar
    clusters:
      infra:
        secrets:
          multibar-alpha-secrets:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--multibar-alpha-password--infra
            - secretKey: username
              remoteRef:
                key: barney--multibar-alpha-username--infra
            secret:
              gitauth.yaml: |
                username:
                  value: {{ .username | toString | b64enc }}
                  encoding: base64
                password:
                  value: {{ .secret | toString | b64enc }}
                  encoding: base64
                ssh-key:
                  value:
                  encoding: base64
                ignore-host-keys: true
          multibar-barnzilla-repos-secrets:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--multibar-barnzilla-repos-password--infra
            - secretKey: username
              remoteRef:
                key: barney--multibar-barnzilla-repos-username--infra
            secret:
              gitauth.yaml: |
                username:
                  value: {{ .username | toString | b64enc }}
                  encoding: base64
                password:
                  value: {{ .secret | toString | b64enc }}
                  encoding: base64
                ssh-key:
                  value:
                  encoding: base64
                ignore-host-keys: true
          multibar-beta-secrets:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--multibar-beta-password--infra
            - secretKey: username
              remoteRef:
                key: barney--multibar-beta-username--infra
            secret:
              gitauth.yaml: |
                username:
                  value: {{ .username | toString | b64enc }}
                  encoding: base64
                password:
                  value: {{ .secret | toString | b64enc }}
                  encoding: base64
                ssh-key:
                  value:
                  encoding: base64
                ignore-host-keys: true
          multibar-firestore-secrets:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--multibar-firestore-secrets--infra
            secret:
              firestore.json: '{{ .secret | toString }}'
          multibar-production-github-secrets:
            data:
            - secretKey: sshkey
              remoteRef:
                key: barney--multibar-production-github-ssh_key--infra
            - secretKey: username
              remoteRef:
                key: barney--multibar-production-github-username--infra
            secret:
              gitauth.yaml: |
                username:
                  value: {{ .username | toString | b64enc }}
                  encoding: base64
                password:
                  value:
                  encoding: base64
                ssh-key:
                  value: {{ .sshkey | toString | b64enc }}
                  encoding: base64
                ignore-host-keys: true
          multibar-production-secrets:
            data:
            - secretKey: secret
              remoteRef:
                key: barney--multibar-production-password--infra
            - secretKey: username
              remoteRef:
                key: barney--multibar-production-username--infra
            secret:
              gitauth.yaml: |
                username:
                  value: {{ .username | toString | b64enc }}
                  encoding: base64
                password:
                  value: {{ .secret | toString | b64enc }}
                  encoding: base64
                ssh-key:
                  value:
                  encoding: base64
                ignore-host-keys: true



  kor:
    source:
      repoURL: https://github.com/aristanetworks/barney-ops
      targetRevision: HEAD
      path: helm-kor/

    syncPolicy:
      automated: {}

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}

    values:
      cronJob:
        enabled: true
        name: barney-kor
        namespace: barney
        schedule: "0 1 * * 1"
        image:
          repository: yonahdissen/kor
          tag: v0.5.2
        command:
          - kor
        args:
          - "configmap"
          - "--include-namespaces=barney"
          - "--include-labels"
          - "app.kubernetes.io/app=bsy"
          - "--older-than=1h"
          - "--delete"
          - "--no-interactive"
        restartPolicy: OnFailure
        successfulJobsHistoryLimit: 2
        failedJobsHistoryLimit: 2
      prometheusExporter:
        enabled: false
      serviceAccount:
        create: true
        name: "barney-kor"
    clusters:
      infra:
        nodeSelector:
          node-role.kubernetes.io/barney: worker

  build-manager-head:
    source:
      repoURL: https://github.com/barney-ci/build-manager
      targetRevision: HEAD
      path: helm/build-manager

    syncPolicy:
      automated: {}

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: api.image.tag
        value: ${ARGOCD_APP_REVISION}
      - name: build-state-agent.image.tag
        value: ${ARGOCD_APP_REVISION}
      - name: desired-state-agent.image.tag
        value: ${ARGOCD_APP_REVISION}
      - name: job-scheduler-agent.image.tag
        value: ${ARGOCD_APP_REVISION}
      - name: elastic-indexer-agent.image.tag
        value: ${ARGOCD_APP_REVISION}
      - name: prometheus-elasticsearch-exporter.extraEnvSecrets.ES_PASSWORD.secret
        value: ${ARGOCD_APP_NAME}-es-elastic-user

    values:
      policyconfig: &policyconfig
        data:
          service-policy.yaml: |
            service-classes:
              # standard build manager
              interactive:
                # TODO: Update this to 2 retries after fixing no-retries
                # policy.
                max-retries: 0
                twin-after: 15m
                max-concurrent: 1
              qualifying:
                max-retries: 8
                twin-after: 30m
                max-concurrent: 2
              merge:
                max-retries: 8
                twin-after: 30m
                max-concurrent: 2
              release:
                max-retries: 8
                twin-after: 30m
                max-concurrent: 3
              # custom policies
              # TODO: TAC model does not currently support arbitrary policies
              # so this policy is known to not work.
              no-retries:
                max-retries: 1
                twin-after: 2h
                max-concurrent: 1
      elasticsearch:
        #TODO: change to barney-build-manager-head after data migration
        #clusterName: "barney"
        nodeCount: 2
        storageClass: portworx-fada
      #Here you can override values for subcharts
      api:
        app:
          #TODO: remove after migration
          collectionRoot: "build-manager"
          es:
            indexName: new-builds
        service:
          port: 8080
        auth:
          state: "optional"
          issuerURL: "https://auth.infra.corp.arista.io"
        apiOverrides:
          X-JobPost-x86_64-Cluster: worker-x86_64-head
          X-JobPost-aarch64-Cluster: worker-aarch64-head

        podAnnotations: &build-manager-head-annotations
          maintainer: barney-dev

          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"

          profiles.grafana.com/memory.scrape: "true"
          profiles.grafana.com/memory.port: "8080"
          profiles.grafana.com/cpu.scrape: "true"
          profiles.grafana.com/cpu.port: "8080"
          profiles.grafana.com/goroutine.scrape: "true"
          profiles.grafana.com/goroutine.port: "8080"
        nodeSelector:
          node-role.kubernetes.io/worker: worker
      job-scheduler-agent:
        app:
          #TODO: remove after migration
          collectionRoot: "build-manager"
          apiOverrides:
            X-JobPost-x86_64-Cluster: worker-x86_64-head
            X-JobPost-aarch64-Cluster: worker-aarch64-head
        apiBaseUrl: "https://barney-api-head.infra.corp.arista.io"
        nodeSelector:
          node-role.kubernetes.io/worker: worker
      build-state-agent:
        app:
          #TODO: remove after migration
          collectionRoot: "build-manager"
        service:
          port: 8080
        podAnnotations: *build-manager-head-annotations
        nodeSelector:
          node-role.kubernetes.io/worker: worker
      desired-state-agent:
        app:
          #TODO: remove after migration
          collectionRoot: "build-manager"
        service:
          port: 8080
        podAnnotations: *build-manager-head-annotations
        nodeSelector:
          node-role.kubernetes.io/worker: worker
      elastic-indexer-agent:
        service:
          port: 8080
        app:
          #TODO: remove after migration
          collectionRoot: "build-manager"
          es:
            indexName: new-builds
        podAnnotations: *build-manager-head-annotations
        nodeSelector:
          node-role.kubernetes.io/worker: worker
      prometheus-elasticsearch-exporter:
        es:
          podAnnotations:
            maintainer: barney-dev

            prometheus.io/path: /metrics
            prometheus.io/port: "9108"
            prometheus.io/scrape: "true"

        secretMounts:
        - name: elastic-certs
          secretName: barney-build-manager-head-infra-es-http-certs-public
          path: /esssl
        - name: client-certs
          secretName: barney-es-prometheus-exporter
          path: /ssl

        nodeSelector:
          node-role.kubernetes.io/worker: worker

    #This is needed as automation expects at least one cluster defined
    clusters:
      infra:
      engprod-dr:
        disable: true


  build-manager-prod:
    source:
      repoURL: https://github.com/barney-ci/build-manager
      targetRevision: b8d230a2a73371f9e4284a1c3fb150552b1923ec
      path: helm/build-manager

    syncPolicy:
      automated: {}

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: api.image.tag
        value: ${ARGOCD_APP_REVISION}
      - name: build-state-agent.image.tag
        value: ${ARGOCD_APP_REVISION}
      - name: desired-state-agent.image.tag
        value: ${ARGOCD_APP_REVISION}
      - name: job-scheduler-agent.image.tag
        value: ${ARGOCD_APP_REVISION}
      - name: elastic-indexer-agent.image.tag
        value: ${ARGOCD_APP_REVISION}
      - name: prometheus-elasticsearch-exporter.extraEnvSecrets.ES_PASSWORD.secret
        value: ${ARGOCD_APP_NAME}-es-elastic-user

    values:
      policyconfig:
        <<: *policyconfig
      elasticsearch:
        nodeCount: 3
        storageClass: portworx-fada
      #Here you can override values for subcharts
      api:
        app:
          es:
            indexName: builds
        service:
          port: 8080
        auth:
          state: "optional"
          issuerURL: "https://auth.infra.corp.arista.io"

        podAnnotations: &build-manager-prod-annotations
          maintainer: barney-dev

          prometheus.io/path: /metrics
          prometheus.io/port: "8080"
          prometheus.io/scrape: "true"

          profiles.grafana.com/memory.scrape: "true"
          profiles.grafana.com/memory.port: "8080"
          profiles.grafana.com/cpu.scrape: "true"
          profiles.grafana.com/cpu.port: "8080"
          profiles.grafana.com/goroutine.scrape: "true"
          profiles.grafana.com/goroutine.port: "8080"
      job-scheduler-agent:
        apiBaseUrl: "https://barney-api.infra.corp.arista.io"
      build-state-agent:
        service:
          port: 8080
        podAnnotations: *build-manager-prod-annotations
      desired-state-agent:
        service:
          port: 8080
        podAnnotations: *build-manager-prod-annotations
      elastic-indexer-agent:
        service:
          port: 8080
        app:
          es:
            indexName: builds
        podAnnotations: *build-manager-prod-annotations
      prometheus-elasticsearch-exporter:
        es:
          podAnnotations:
            maintainer: barney-dev

            prometheus.io/path: /metrics
            prometheus.io/port: "9108"
            prometheus.io/scrape: "true"

        secretMounts:
        - name: elastic-certs
          secretName: barney-build-manager-prod-infra-es-http-certs-public
          path: /esssl
        - name: client-certs
          secretName: barney-es-prometheus-exporter
          path: /ssl

        nodeSelector:
          node-role.kubernetes.io/worker: worker

    #This is needed as automation expects at least one cluster defined
    clusters:
      infra:
        build-state-agent:
          nodeSelector:
            node-role.kubernetes.io/worker: worker
        desired-state-agent:
          nodeSelector:
            node-role.kubernetes.io/worker: worker
        job-scheduler-agent:
          nodeSelector:
            node-role.kubernetes.io/worker: worker
        elastic-indexer-agent:
          nodeSelector:
            node-role.kubernetes.io/worker: worker
        api:
          nodeSelector:
            node-role.kubernetes.io/worker: worker
      engprod-dr:
        elasticsearch:
          nodeCount: 1
        elastic:
          storageClass: "standard-rwo"
        build-state-agent:
          image:
            repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/build-manager--build-state-agent
          nodeSelector:
              node-role/worker: worker
        desired-state-agent:
          image:
            repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/build-manager--desired-state-agent
          nodeSelector:
            node-role/worker: worker
        job-scheduler-agent:
          image:
            repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/build-manager--job-scheduler-agent
          apiBaseUrl: "https://barney-api.engprod-dr.corp.arista.io"
          nodeSelector:
            node-role/worker: worker
        elastic-indexer-agent:
          image:
            repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/build-manager--elastic-indexer-agent
          nodeSelector:
            node-role/worker: worker
        api:
          image:
            repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/build-manager--api
          nodeSelector:
            node-role/worker: worker
          auth:
            state: "off"
            issuerURL: "https://auth.engprod-dr.corp.arista.io"
          #secret:
          #  name: build-manager-aeris-token
        #for engprod-dr deployment we don't need any observability
        #so scale it down to 0 and disable everything we can
        prometheus-elasticsearch-exporter:
          service:
            enabled: false
          replicaCount: 0


  eck-operator:
    maintainers:
    - username: barney-dev
      email: barney-dev@arista.com

    source:
      chart: eck-operator
      repoURL: https://helm.elastic.co
      targetRevision: 2.10.0

    values:
      managedNamespaces:
        - barney

    clusters:
      #only deploy to engprod-dr, infra deployment is managed by argocd-apps
      infra:
        disable: true
      engprod-dr:

  cache-blue:
    source:
      repoURL: https://github.com/barney-ci/barney
      targetRevision: f551ba1aacea0149748aaab1ff8c36cff388c258
      path: helm/cache

    # clusterSource extends "source" above with per-cluster parameters.
    clusterSource:
      engprod-dr:
        targetRevision: 86f2bffc474556f3704fcb5b9f5b346379a0858f

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--arch-x86_64

    syncPolicy:
      automated: {}

    values: &cache_defaults
      image:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/barney

      config:
        maxDownloadWorkers: 100
        httpIdleTimeout: "30s"
        maxHints: 25000000
      cache:
        maxDeletionWorkers: 100
        threshold: 85%
        usageWatermarks:
          high: 95%
          low: 90%

      podAnnotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "8184"
        prometheus.io/scrape: "true"

        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/memory.port: "8184"
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/cpu.port: "8184"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/goroutine.port: "8184"

      service:
        gcAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8182"
          prometheus.io/scrape: "true"

      influxDBAddr: planck.sjc.aristanetworks.com:8107

    clusters:
      infra: &cluster_defaults
        nodeSelector:
          node-role.kubernetes.io/barney-cache-service: barney-cache-service

        # cache_blue_green
        # This deployment is _shrinking_ to 0
        replicas: 13

        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            maxUnavailable: 1
            maxSurge: 1

        labels:
          app.kubernetes.io/component: cache-service

        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule

        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/app
                      operator: In
                      values:
                        - cache
                topologyKey: "kubernetes.io/hostname"

        cache:
          nodePath: /data2/b5cache

        config:
          dnsName: barney-cache-service
          maxHints: 10000000
          replicationFactor: 3

      # engprod-dr cache pods are managed by StatefylSet, there is no need to switch between green/blue
      # deployments in this case.
      engprod-dr:
        replicas: 2

        image:
          repository: artifactory-dr-barney-docker.engprod-dr.corp.arista.io/barney.ci/barney

        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            partition: 0

        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/app
                      operator: In
                      values:
                        - cache
                        - bsy
                topologyKey: "kubernetes.io/hostname"

        config:
          dnsName: barney-cache-blue

        cache:
          volumeClaim:
            enabled: true
            storageClass: standard-rwo
            size: 200Gi


  cache-green:
    source:
      repoURL: https://github.com/barney-ci/barney
      targetRevision: 66eecbbfd4e4ed227df8b11727555fdd4d055503
      path: helm/cache

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--arch-x86_64

    syncPolicy:
      automated: {}

    values:
      <<: *cache_defaults

    clusters:
      infra:
        <<: *cluster_defaults

        # cache_blue_green
        # This deployment is _growing_ to 13
        replicas: 0

  cache-head:
    source:
      repoURL: https://github.com/barney-ci/barney
      targetRevision: b4081cf1214871d9b5cda00dcf866bdc575c0b4e
      path: helm/cache

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--multi-arch

    syncPolicy:
      automated: {}

    values:
      <<: *cache_defaults

      cache:
        maxDeletionWorkers: 100
        threshold: 75%

    clusters:
      infra:
        image:
          repository: barney-docker.infra.corp.arista.io/barney.ci/barney

        hostPort: 20001

        replicas: 2

        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/name
                      operator: In
                      values:
                        - barney-cache-head
                topologyKey: "kubernetes.io/hostname"

        nodeSelector:
          node-role.kubernetes.io/barney-test-cache-manager: barney-test-cache-manager

        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            maxUnavailable: 1
            maxSurge: 0

        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule

        cache:
          nodePath: /data2/b5cache/barney-bsy-head

        config:
          dnsName: barney-cache-head
          maxHints: 10000

  # local cache manager for bsy-head-smallcache
  cache-head-smallcache:
    source:
      repoURL: https://github.com/barney-ci/barney
      targetRevision: HEAD
      path: helm/cache

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--multi-arch

    syncPolicy:
      automated: {}

    values:
      <<: *cache_defaults

      cache:
        maxDeletionWorkers: 100
        threshold: 75%
        usageWatermarks:
          high: 80%
          low: 75%

    clusters:
      infra:
        image:
          repository: barney-docker.infra.corp.arista.io/barney.ci/barney

        cacheService: false
        hostPort: 20002

        replicas: 1

        nodeSelector:
          node-role.kubernetes.io/barney-smallcache: barney-smallcache

        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            maxUnavailable: 1
            maxSurge: 1

        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule

        cache:
          nodePath: /data2/b5cache/smallcache/barney-bsy-head-smallcache
          threshold: 40GB

        config:
          dnsName: barney-cache-head

        pod:
          cache:
            resources:
              limits:
                memory: "10Gi"
          gc:
            resources:
              limits:
                memory: "1Gi"

  cache-local-blue:
    source:
      repoURL: https://github.com/barney-ci/barney
      targetRevision: a60f4317e8101eaf1db4d5ee649373daa6beb30b
      path: helm/cache

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--multi-arch

    syncPolicy:
      automated: {}

    values: &cache_local_defaults
      cache:
        maxDeletionWorkers: 100

      image:
        repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/barney

      podAnnotations:
        prometheus.io/path: /metrics
        prometheus.io/port: "8184"
        prometheus.io/scrape: "true"

        profiles.grafana.com/memory.scrape: "true"
        profiles.grafana.com/memory.port: "8184"
        profiles.grafana.com/cpu.scrape: "true"
        profiles.grafana.com/cpu.port: "8184"
        profiles.grafana.com/goroutine.scrape: "true"
        profiles.grafana.com/goroutine.port: "8184"

      service:
        gcAnnotations:
          prometheus.io/path: /metrics
          prometheus.io/port: "8182"
          prometheus.io/scrape: "true"

      influxDBAddr: planck.sjc.aristanetworks.com:8107

    clusters:
      infra: &cache_local_infra_defaults
        # temporary change of image repository, it's caused by 'unknown blob' error
        image:
          repository: barney-docker.infra.corp.arista.io/barney.ci/barney

        # This deployment is _growing_ to 39
        replicas: 15 # CACHE_LOCAL_BLUE_GREEN

        hostPort: 20001
        cacheService: false

        nodeSelector:
          node-role.kubernetes.io/barney-cache-local: barney-cache-local

        updateStrategy:
          type: RollingUpdate
          rollingUpdate:
            maxUnavailable: 1
            maxSurge: 0

        tolerations:
          - key: type
            operator: Equal
            value: barney
            effect: NoSchedule

        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                    - key: app.kubernetes.io/app
                      operator: In
                      values:
                        - cache
                topologyKey: "kubernetes.io/hostname"

        cache:
          nodePath: /data2/b5cache/barney-bsy

        config:
          dnsName: barney-cache-service
          replicationFactor: 3
          httpIdleTimeout: "30s"
          maxHints: 100

        pod:
          cache:
            resources:
              limits:
                memory: "16Gi"
              requests:
                memory: "8Gi"
            addEnv:
              - name: GOMEMLIMIT
                value: "14GiB"
          gc:
            resources:
              limits:
                memory: "20Gi"
              requests:
                memory: "8Gi"

  cache-local-green:
    source:
      repoURL: https://github.com/barney-ci/barney
      targetRevision: f551ba1aacea0149748aaab1ff8c36cff388c258
      path: helm/cache

    parameters:
      # NOTE: ARGOCD_APP_REVISION has to be specified as parameter, because argocd
      # is unable to substitute in a `values` string.
      - name: image.tag
        value: ${ARGOCD_APP_REVISION}--multi-arch

    syncPolicy:
      automated: {}

    values:
      <<: *cache_local_defaults

    clusters:
      infra:
        <<: *cache_local_infra_defaults

        image:
          repository: artifactory-barney-docker.infra.corp.arista.io/barney.ci/barney

        # This deployment is _shrinking_ to 0
        replicas: 24 # CACHE_LOCAL_BLUE_GREEN


  barnzilla-repocleaner:
    source:
      repoURL: https://github.com/aristanetworks/barney-ops
      targetRevision: HEAD
      path: helm-repocleaner
    syncPolicy:
      automated: {}
    values:
      app:
        targetPod: "horseland-gerrit-0"
        deleteOlderThan: "30 days ago"
        gitRepo: "barnzilla-repos.git"
        refs:
          - refs/builds
          - refs/muts
          - refs/publishedChangeNum
    clusters:
      infra:

  b5-export-store-repocleaner:
    source:
      repoURL: https://github.com/aristanetworks/barney-ops
      targetRevision: HEAD
      path: helm-repocleaner
    syncPolicy:
      automated: {}
    values:
      app:
        targetPod: "horseland-gerrit-0"
        deleteOlderThan: "14 days ago"
        gitRepo: "b5-export-store.git"
        refs:
          - refs/notes/b5/exported
        extraCommands: |
          echo "delete ${ref/"refs/notes/b5/exported/"/"refs/b5/exported/"}"
          echo "delete ${ref/"refs/notes/b5/exported/"/"refs/b5/exported-repomap/"}"
    clusters:
      infra:
